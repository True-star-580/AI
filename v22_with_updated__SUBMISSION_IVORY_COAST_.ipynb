{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "name": "v22_with updated_ SUBMISSION_IVORY_COAST_",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 11852884,
          "sourceType": "datasetVersion",
          "datasetId": 7447863
        },
        {
          "sourceId": 12007838,
          "sourceType": "datasetVersion",
          "datasetId": 7554220
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "61f02bfd68ee4b1ba11964d7e52b0601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3175e632e2634aecba5842d44194476f"
            ],
            "layout": "IPY_MODEL_1e31c01feaf74aff92b0a9a79594aaf7"
          }
        },
        "8321633e8a6b494d911d265261134620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fceeab1bb486453b8d251c7751b9d9d1",
            "placeholder": "​",
            "style": "IPY_MODEL_7a5bbac0c91a4e6c927bffea5ccf5eba",
            "value": "<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"
          }
        },
        "4d3c306845b84d1091778957e2a90226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_18ff672e8f9f480eb825502d19f3009a",
            "placeholder": "​",
            "style": "IPY_MODEL_ce225926396644658d9d163faaf1ff4f",
            "value": "paulnkamau"
          }
        },
        "35045637531f46eaa254d8248ec51dfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8480ccadb3914b6b988ad37f85730724",
            "placeholder": "​",
            "style": "IPY_MODEL_f656adae0c524b7fb293da340393db5b",
            "value": ""
          }
        },
        "329200f56b45485a9f87c412fbc5b7fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_09c809afb3e54374bd36a10602e9e884",
            "style": "IPY_MODEL_a609d09eb2fd4592ab6bdbb600cec3d9",
            "tooltip": ""
          }
        },
        "91e95dc3b7614388802b4c48e80065a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51f19c6b96b74487a8119938becc7423",
            "placeholder": "​",
            "style": "IPY_MODEL_d4e38dcfe1344c49b8e0ef70f09582f7",
            "value": "\n<b>Thank You</b></center>"
          }
        },
        "1e31c01feaf74aff92b0a9a79594aaf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "fceeab1bb486453b8d251c7751b9d9d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a5bbac0c91a4e6c927bffea5ccf5eba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18ff672e8f9f480eb825502d19f3009a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce225926396644658d9d163faaf1ff4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8480ccadb3914b6b988ad37f85730724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f656adae0c524b7fb293da340393db5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09c809afb3e54374bd36a10602e9e884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a609d09eb2fd4592ab6bdbb600cec3d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "51f19c6b96b74487a8119938becc7423": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4e38dcfe1344c49b8e0ef70f09582f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2d48c29ecd64f0b8164662cb3611d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_833533cb1e7e4a949677126212df2ba6",
            "placeholder": "​",
            "style": "IPY_MODEL_cf38663d89874fc0a69d2266fe409159",
            "value": "Connecting..."
          }
        },
        "833533cb1e7e4a949677126212df2ba6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf38663d89874fc0a69d2266fe409159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3175e632e2634aecba5842d44194476f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d11eb9b4343d4f91a23f1ee7112e9351",
            "placeholder": "​",
            "style": "IPY_MODEL_e4b7d905b63d4fa5a54858af3068e50d",
            "value": "Kaggle credentials successfully validated."
          }
        },
        "d11eb9b4343d4f91a23f1ee7112e9351": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4b7d905b63d4fa5a54858af3068e50d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2a56b85ab21428c8c3d5c577d114596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28fe449f460d4fd993335bcc61495af5",
              "IPY_MODEL_93956e2e6dbd4e23a6cf597c4ae15357",
              "IPY_MODEL_6d09186d7fbb45bcb1ed457e8aa9a956"
            ],
            "layout": "IPY_MODEL_7a3d955d90d54692b0006ae3f0bd92fb"
          }
        },
        "28fe449f460d4fd993335bcc61495af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a1deb195c3a470ab7d1b78d01d84eac",
            "placeholder": "​",
            "style": "IPY_MODEL_b0d28871ee3641dcade2b29faa05c56e",
            "value": "Processing Tiles: 100%"
          }
        },
        "93956e2e6dbd4e23a6cf597c4ae15357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d15703ddb81941338c25e608d7ab32f2",
            "max": 2859,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76af2a8a0eb845989e71cd7b09f88cfc",
            "value": 2859
          }
        },
        "6d09186d7fbb45bcb1ed457e8aa9a956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8831764a656b440496e58ef3a9943830",
            "placeholder": "​",
            "style": "IPY_MODEL_efff88f76bf54953a2bd7e79418f8e55",
            "value": " 2859/2859 [00:21&lt;00:00, 136.32file/s]"
          }
        },
        "7a3d955d90d54692b0006ae3f0bd92fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a1deb195c3a470ab7d1b78d01d84eac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0d28871ee3641dcade2b29faa05c56e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d15703ddb81941338c25e608d7ab32f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76af2a8a0eb845989e71cd7b09f88cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8831764a656b440496e58ef3a9943830": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efff88f76bf54953a2bd7e79418f8e55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04ab451cc1a54ca4a7da384bb600d7a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc47fbb8c8a544d18ccf179f39c90e2d",
              "IPY_MODEL_b6f86fc0eafa49998436d0f5f552f5a0",
              "IPY_MODEL_ffc1e47466214537a8ced98a8c03aaf0"
            ],
            "layout": "IPY_MODEL_3c7672bef5834eab83b1df539f271871"
          }
        },
        "dc47fbb8c8a544d18ccf179f39c90e2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_336a8f1a243c44158005e498950d0910",
            "placeholder": "​",
            "style": "IPY_MODEL_b9e6cd3672614d4eaeaf138212d99cae",
            "value": "Processing Tiles: 100%"
          }
        },
        "b6f86fc0eafa49998436d0f5f552f5a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51dacc2113bb4753b81e33acd5380798",
            "max": 564,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2db9f0f453e484db5273238d47c4dca",
            "value": 564
          }
        },
        "ffc1e47466214537a8ced98a8c03aaf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18c02e55f2b44e948037564dc7da5fee",
            "placeholder": "​",
            "style": "IPY_MODEL_fa99af2d923c43c0b960c3f16a858fea",
            "value": " 564/564 [00:02&lt;00:00, 274.24file/s]"
          }
        },
        "3c7672bef5834eab83b1df539f271871": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "336a8f1a243c44158005e498950d0910": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9e6cd3672614d4eaeaf138212d99cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51dacc2113bb4753b81e33acd5380798": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2db9f0f453e484db5273238d47c4dca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18c02e55f2b44e948037564dc7da5fee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa99af2d923c43c0b960c3f16a858fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/True-star-580/AI/blob/main/v22_with_updated__SUBMISSION_IVORY_COAST_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "Ck7VB9kLpW9k",
        "outputId": "8b3d2097-c59e-42e9-860f-bf89d033bef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "61f02bfd68ee4b1ba11964d7e52b0601",
            "8321633e8a6b494d911d265261134620",
            "4d3c306845b84d1091778957e2a90226",
            "35045637531f46eaa254d8248ec51dfc",
            "329200f56b45485a9f87c412fbc5b7fd",
            "91e95dc3b7614388802b4c48e80065a4",
            "1e31c01feaf74aff92b0a9a79594aaf7",
            "fceeab1bb486453b8d251c7751b9d9d1",
            "7a5bbac0c91a4e6c927bffea5ccf5eba",
            "18ff672e8f9f480eb825502d19f3009a",
            "ce225926396644658d9d163faaf1ff4f",
            "8480ccadb3914b6b988ad37f85730724",
            "f656adae0c524b7fb293da340393db5b",
            "09c809afb3e54374bd36a10602e9e884",
            "a609d09eb2fd4592ab6bdbb600cec3d9",
            "51f19c6b96b74487a8119938becc7423",
            "d4e38dcfe1344c49b8e0ef70f09582f7",
            "a2d48c29ecd64f0b8164662cb3611d43",
            "833533cb1e7e4a949677126212df2ba6",
            "cf38663d89874fc0a69d2266fe409159",
            "3175e632e2634aecba5842d44194476f",
            "d11eb9b4343d4f91a23f1ee7112e9351",
            "e4b7d905b63d4fa5a54858af3068e50d"
          ]
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61f02bfd68ee4b1ba11964d7e52b0601"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kaggle credentials set.\n",
            "Kaggle credentials successfully validated.\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "paulnkamau_ivorycoast_byte_sizedagric_path = kagglehub.dataset_download('paulnkamau/ivorycoast-byte-sizedagric')\n",
        "paulnkamau_cleaned_frm_v1_v21_path = kagglehub.dataset_download('paulnkamau/cleaned-frm-v1-v21')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "OyXOMEZMpW9p",
        "outputId": "10a13163-6acd-4e91-f8a1-beab1bf8ac88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/paulnkamau/ivorycoast-byte-sizedagric?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 590M/590M [00:04<00:00, 134MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/paulnkamau/cleaned-frm-v1-v21?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.95M/4.95M [00:00<00:00, 95.4MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Data source import complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(os.path.join(paulnkamau_cleaned_frm_v1_v21_path, 'train_cleaned.csv'))\n",
        "test = pd.read_csv(os.path.join(paulnkamau_cleaned_frm_v1_v21_path, 'test_cleaned.csv'))\n",
        "\n",
        "print(\"Train and test CSVs loaded successfully.\")\n",
        "\n",
        "MAX_TILE_SIZE = 326 # Keep this definition"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T14:38:55.890753Z",
          "iopub.execute_input": "2025-05-30T14:38:55.891571Z",
          "iopub.status.idle": "2025-05-30T14:38:55.896552Z",
          "shell.execute_reply.started": "2025-05-30T14:38:55.891544Z",
          "shell.execute_reply": "2025-05-30T14:38:55.895622Z"
        },
        "id": "_sfXYFHjpW9p",
        "outputId": "595649f7-6e58-4efc-91af-5b9b73e00c8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and test CSVs loaded successfully.\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install autogluon -q"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T14:38:55.898547Z",
          "iopub.execute_input": "2025-05-30T14:38:55.899008Z",
          "iopub.status.idle": "2025-05-30T14:38:55.94892Z",
          "shell.execute_reply.started": "2025-05-30T14:38:55.898966Z",
          "shell.execute_reply": "2025-05-30T14:38:55.94789Z"
        },
        "id": "EM2hPxTtpW9q"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio geopandas -q"
      ],
      "metadata": {
        "id": "2umBZVVwIof4",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T14:38:55.949961Z",
          "iopub.execute_input": "2025-05-30T14:38:55.950281Z",
          "iopub.status.idle": "2025-05-30T14:39:04.121459Z",
          "shell.execute_reply.started": "2025-05-30T14:38:55.950258Z",
          "shell.execute_reply": "2025-05-30T14:39:04.1203Z"
        },
        "outputId": "c7d9b080-0271-4342-a27f-30e5e7755791",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.6/323.6 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.8/27.8 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m130.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import geopandas as gpd\n",
        "import gc"
      ],
      "metadata": {
        "id": "gVaDjRwFI-Rl",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T14:39:04.122688Z",
          "iopub.execute_input": "2025-05-30T14:39:04.123029Z",
          "iopub.status.idle": "2025-05-30T14:39:06.250039Z",
          "shell.execute_reply.started": "2025-05-30T14:39:04.122997Z",
          "shell.execute_reply": "2025-05-30T14:39:06.249147Z"
        }
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm plotly -q"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T14:39:06.421655Z",
          "iopub.execute_input": "2025-05-30T14:39:06.422018Z",
          "iopub.status.idle": "2025-05-30T14:39:10.150031Z",
          "shell.execute_reply.started": "2025-05-30T14:39:06.421989Z",
          "shell.execute_reply": "2025-05-30T14:39:10.148893Z"
        },
        "id": "KciOMi_bpW9t"
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-image -q\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T14:39:10.151882Z",
          "iopub.execute_input": "2025-05-30T14:39:10.152159Z",
          "iopub.status.idle": "2025-05-30T14:39:14.355269Z",
          "shell.execute_reply.started": "2025-05-30T14:39:10.152132Z",
          "shell.execute_reply": "2025-05-30T14:39:14.353606Z"
        },
        "id": "NaxNcyWppW9t"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "# disti=pd.crosstab(train_jan['month'], train_jan['class'])\n",
        "# disti"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T14:39:14.356888Z",
          "iopub.execute_input": "2025-05-30T14:39:14.357234Z",
          "iopub.status.idle": "2025-05-30T14:39:14.362467Z",
          "shell.execute_reply.started": "2025-05-30T14:39:14.357201Z",
          "shell.execute_reply": "2025-05-30T14:39:14.361513Z"
        },
        "id": "5Vy7LabDpW9t"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.feature import graycomatrix, graycoprops\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T14:39:14.363726Z",
          "iopub.execute_input": "2025-05-30T14:39:14.36409Z",
          "iopub.status.idle": "2025-05-30T14:39:14.563063Z",
          "shell.execute_reply.started": "2025-05-30T14:39:14.36406Z",
          "shell.execute_reply": "2025-05-30T14:39:14.561891Z"
        },
        "id": "QY2IZ_kvpW9u"
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T14:39:14.564358Z",
          "iopub.execute_input": "2025-05-30T14:39:14.565354Z",
          "iopub.status.idle": "2025-05-30T14:39:14.691541Z",
          "shell.execute_reply.started": "2025-05-30T14:39:14.56532Z",
          "shell.execute_reply": "2025-05-30T14:39:14.690545Z"
        },
        "id": "ycFvbCq3pW9u",
        "outputId": "ae43d492-90bd-41f5-b182-8974f26bc90c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----"
      ],
      "metadata": {
        "id": "Ri-E3zq4pW9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "-------"
      ],
      "metadata": {
        "id": "uBwCtWqEpW9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Modified process_and_return_tiles to return processed DataFrame subset ---\n",
        "import os\n",
        "import rasterio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rasterio.windows import Window"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T14:39:14.834148Z",
          "iopub.execute_input": "2025-05-30T14:39:14.83446Z",
          "iopub.status.idle": "2025-05-30T14:39:15.963818Z",
          "shell.execute_reply.started": "2025-05-30T14:39:14.83443Z",
          "shell.execute_reply": "2025-05-30T14:39:15.962826Z"
        },
        "id": "Utqh0J0lpW9w"
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import rasterio\n",
        "import numpy as np # Need numpy for seterr\n",
        "import pandas as pd\n",
        "from rasterio.windows import Window\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import plotly.express as px\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from scipy import ndimage\n",
        "from skimage.feature import graycomatrix, graycoprops\n",
        "from skimage.filters import gabor\n",
        "from skimage.measure import shannon_entropy\n",
        "from skimage.feature import local_binary_pattern\n",
        "from scipy.ndimage import binary_opening, binary_closing, binary_erosion, binary_dilation\n",
        "from skimage.filters.rank import entropy # Need import specifically for rank\n",
        "from skimage.morphology import disk # Need import specifically for disk\n",
        "from skimage.filters import sobel, prewitt, roberts\n",
        "from skimage.feature import canny # Canny is in feature\n",
        "from skimage.filters import gaussian # Sometimes used before edge detection\n",
        "\n",
        "# Import tqdm for progress bars\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Import joblib for parallel processing\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# import warnings # Not strictly needed if using np.seterr instead\n",
        "\n",
        "# TILE_SIZE must be defined, e.g., TILE_SIZE = 64 or 128 or MAX_TILE_SIZE if MAX_TILE_SIZE is defined elsewhere.\n",
        "# Assuming MAX_TILE_SIZE is defined globally or imported.\n",
        "# Example placeholder if not defined elsewhere:\n",
        "# MAX_TILE_SIZE = 128 # <--- Define this or ensure it's defined elsewhere\n",
        "TILE_SIZE = MAX_TILE_SIZE # <--- Define this or ensure it's defined elsewhere\n",
        "\n",
        "# Define TILE_SIZE if not defined elsewhere for this example\n",
        "# Remove these lines if MAX_TILE_SIZE/TILE_SIZE are truly defined elsewhere\n",
        "try:\n",
        "    TILE_SIZE\n",
        "except NameError:\n",
        "    print(\"TILE_SIZE not defined. Using default 128.\")\n",
        "    TILE_SIZE = 128\n",
        "\n",
        "\n",
        "# --- Feature Extraction Functions (Keeping these consistent with previous response) ---\n",
        "# (Added robustness checks like epsilon denominators and np.maximum already)\n",
        "\n",
        "def extract_ndvi(nir_band, red_band, epsilon=1e-8):\n",
        "    \"\"\" Calculate NDVI from NIR and Red bands \"\"\"\n",
        "    # Ensure inputs are float to handle division and potential NaN/Inf\n",
        "    nir_band = nir_band.astype(np.float32)\n",
        "    red_band = red_band.astype(np.float32)\n",
        "    # Avoid division by zero/near-zero\n",
        "    denominator = nir_band + red_band\n",
        "    return (nir_band - red_band) / (denominator + epsilon)\n",
        "\n",
        "def extract_savi(nir_band, red_band, L=0.5, epsilon=1e-8):\n",
        "    \"\"\" Calculate SAVI from NIR and Red bands \"\"\"\n",
        "    nir_band = nir_band.astype(np.float32)\n",
        "    red_band = red_band.astype(np.float32)\n",
        "    # Avoid division by zero/near-zero\n",
        "    denominator = nir_band + red_band + L\n",
        "    return ((nir_band - red_band) * (1 + L)) / (denominator + epsilon)\n",
        "\n",
        "def extract_evi(nir_band, red_band, blue_band, G=2.5, C1=6, C2=7.5, L=10000, epsilon=1e-8):\n",
        "    \"\"\" Calculate EVI from NIR, Red, and Blue bands \"\"\"\n",
        "    nir_band = nir_band.astype(np.float32)\n",
        "    red_band = red_band.astype(np.float32)\n",
        "    blue_band = blue_band.astype(np.float32)\n",
        "    denominator = (nir_band + C1 * red_band - C2 * blue_band + L)\n",
        "    # Avoid division by zero/near-zero\n",
        "    return G * ((nir_band - red_band) / (denominator + epsilon))\n",
        "\n",
        "def extract_s2rep(b4, b5, b6, b7, epsilon=1e-8):\n",
        "    \"\"\"Calculate S2REP from Red (B4) and Red Edge (B5, B6, B7) bands.\"\"\"\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b5 = b5.astype(np.float32)\n",
        "    b6 = b6.astype(np.float32)\n",
        "    b7 = b7.astype(np.float32)\n",
        "    numerator = (((b7 + b4) / 2) - b5)\n",
        "    denominator = (b6 - b5)\n",
        "    # Handle case where b6 == b5\n",
        "    safe_denominator = np.where(np.abs(denominator) < epsilon, epsilon * np.sign(denominator + epsilon), denominator)\n",
        "    s2rep = 705 + 35 * (numerator / safe_denominator)\n",
        "    return s2rep\n",
        "\n",
        "def extract_ccci(b4, b5, b8, epsilon=1e-8):\n",
        "    \"\"\"Calculate CCCI from NIR (B8), Red (B4), and Red Edge (B5) bands.\"\"\"\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b5 = b5.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    ndre_denominator = (b8 + b5)\n",
        "    ndre = (b8 - b5) / (ndre_denominator + epsilon)\n",
        "    ndvi_denominator = (b8 + b4)\n",
        "    ndvi = (b8 - b4) / (ndvi_denominator + epsilon)\n",
        "    return ndre / (ndvi + epsilon) # Avoid division by zero ndvi\n",
        "\n",
        "def extract_ndwi(b3, b8, epsilon=1e-8):\n",
        "    \"\"\"Calculate NDWI from Green (B3) and NIR (B8) bands.\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    denominator = (b3 + b8)\n",
        "    return (b3 - b8) / (denominator + epsilon)\n",
        "\n",
        "# Original vegetation indices\n",
        "def extract_ndre(b8, b6, epsilon=1e-8):\n",
        "    \"\"\"Normalized Difference Red Edge\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b6 = b6.astype(np.float32)\n",
        "    denominator = (b8 + b6)\n",
        "    return (b8 - b6) / (denominator + epsilon)\n",
        "\n",
        "def extract_mmsr(b8, b4, epsilon=1e-8):\n",
        "    \"\"\"Modified Modified Simple Ratio\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    ratio_denominator = (b4)\n",
        "    ratio = b8 / (ratio_denominator + epsilon)\n",
        "    # Added epsilon to sqrt\n",
        "    return (ratio - 1) / (np.sqrt(np.maximum(ratio, 0.0) + epsilon) + 1) # Added maximum for safety\n",
        "\n",
        "def extract_gndvi(b8, b3, epsilon=1e-8):\n",
        "    \"\"\"Green Normalized Difference Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    denominator = (b8 + b3)\n",
        "    return (b8 - b3) / (denominator + epsilon)\n",
        "\n",
        "def extract_evi2(b8, b4, epsilon=1e-8):\n",
        "    \"\"\"Enhanced Vegetation Index 2\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    denominator = (b8 + 2.4 * b4 + 1)\n",
        "    return 2.5 * (b8 - b4) / (denominator + epsilon)\n",
        "\n",
        "def extract_ngrdi(b3, b4, epsilon=1e-8):\n",
        "    \"\"\"Normalized Green Red Difference Index\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    denominator = (b3 + b4)\n",
        "    return (b3 - b4) / (denominator + epsilon)\n",
        "\n",
        "def extract_mndwi(b3, b11, epsilon=1e-8):\n",
        "    \"\"\"Modified Normalized Difference Water Index\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b11 = b11.astype(np.float32)\n",
        "    denominator = (b3 + b11)\n",
        "    return (b3 - b11) / (denominator + epsilon)\n",
        "\n",
        "def extract_osavi(b8, b4, epsilon=1e-8):\n",
        "    \"\"\"Optimized Soil Adjusted Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    denominator = (b8 + b4 + 0.16)\n",
        "    return (b8 - b4) / (denominator + epsilon)\n",
        "\n",
        "def extract_wdrvi(b8, b4, alpha=0.1, epsilon=1e-8):\n",
        "    \"\"\"Wide Dynamic Range Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    denominator = (alpha * b8 + b4)\n",
        "    return (alpha * b8 - b4) / (denominator + epsilon)\n",
        "\n",
        "def extract_tgi(b3, b4, b2):\n",
        "    \"\"\"Triangular Greenness Index\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b2 = b2.astype(np.float32)\n",
        "    return b3 - 0.39 * b4 - 0.61 * b2\n",
        "\n",
        "def extract_gcvi(b8, b3, epsilon=1e-8):\n",
        "    \"\"\"Green Chlorophyll Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    return (b8 / (b3 + epsilon)) - 1\n",
        "\n",
        "def extract_rgvi(b1, b3, b4, b5, b7, epsilon=1e-8):\n",
        "    \"\"\"Red Green Vegetation Index\"\"\"\n",
        "    b1 = b1.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b5 = b5.astype(np.float32)\n",
        "    b7 = b7.astype(np.float32)\n",
        "    denominator = (b4 + b5 + b7)\n",
        "    return 1 - ((b1 + b3) / (denominator + epsilon))\n",
        "\n",
        "def extract_arvi(b8, b4, b2, gamma=1.0, epsilon=1e-8):\n",
        "    \"\"\"Atmospherically Resistant Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b2 = b2.astype(np.float32)\n",
        "    # Using user's simplified formula: (b8 - (2 * b4) + b2) / (b8 + (2 * b4) + b2 + epsilon)\n",
        "    numerator = b8 - (2 * b4) + b2\n",
        "    denominator = b8 + (2 * b4) + b2\n",
        "    return numerator / (denominator + epsilon)\n",
        "\n",
        "\n",
        "def extract_sipi(b8, b2, b4, epsilon=1e-8):\n",
        "    \"\"\"Structure Insensitive Pigment Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    denominator = (b8 - b4)\n",
        "    safe_denominator = np.where(np.abs(denominator) < epsilon, epsilon * np.sign(denominator + epsilon), denominator)\n",
        "    return (b8 - b2) / safe_denominator\n",
        "\n",
        "def extract_rendvi(b6, b5, epsilon=1e-8):\n",
        "    \"\"\"Red Edge Normalized Difference Vegetation Index\"\"\"\n",
        "    b6 = b6.astype(np.float32)\n",
        "    b5 = b5.astype(np.float32)\n",
        "    denominator = (b6 + b5)\n",
        "    return (b6 - b5) / (denominator + epsilon)\n",
        "\n",
        "def extract_mresr(b6, b1, b5, epsilon=1e-8):\n",
        "    \"\"\"Modified Red Edge Simple Ratio\"\"\"\n",
        "    b6 = b6.astype(np.float32)\n",
        "    b1 = b1.astype(np.float32)\n",
        "    b5 = b5.astype(np.float32)\n",
        "    denominator = (b5 - b1)\n",
        "    safe_denominator = np.where(np.abs(denominator) < epsilon, epsilon * np.sign(denominator + epsilon), denominator)\n",
        "    return (b6 - b1) / safe_denominator\n",
        "\n",
        "def extract_ryi(b3, b2, epsilon=1e-8):\n",
        "    \"\"\"Red Yellow Index\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b2 = b2.astype(np.float32)\n",
        "    return b3 / (b2 + epsilon)\n",
        "\n",
        "def extract_ndyi(b3, b2, epsilon=1e-8):\n",
        "    \"\"\"Normalized Difference Yellowness Index\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b2 = b2.astype(np.float32)\n",
        "    denominator = (b3 + b2)\n",
        "    return (b3 - b2) / (denominator + epsilon)\n",
        "\n",
        "def extract_dyi(b3, b2):\n",
        "    \"\"\"Difference Yellowness Index\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b2 = b2.astype(np.float32)\n",
        "    return b3 - b2\n",
        "\n",
        "def extract_aci(b8, b4, b3): # Removed epsilon from formula, it's a product\n",
        "    \"\"\"Ashburn Chlorophyll Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    # User's formula: b8 * (b4 + b3)\n",
        "    return b8 * (b4 + b3)\n",
        "\n",
        "def extract_cvi(b8, b3, b4, epsilon=1e-8):\n",
        "    \"\"\"Chlorophyll Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    return (b8 / (b3 + epsilon)) * (b4 / (b3 + epsilon))\n",
        "\n",
        "def extract_avi(b8, b4):\n",
        "    \"\"\"Advanced Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    return b8 * (1 - b4) * (b8 - b4)\n",
        "\n",
        "def extract_si(b2, b3, b4):\n",
        "    \"\"\"Shadow Index\"\"\"\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    return (1 - b2) * (1 - b3) * (1 - b4)\n",
        "\n",
        "def extract_bsi(b11, b4, b8, b2, epsilon=1e-8):\n",
        "    \"\"\"Bare Soil Index\"\"\"\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b2 = b2.astype(np.float32)\n",
        "    denominator = ((b11 + b4) + (b8 + b2))\n",
        "    return ((b11 + b4) - (b8 + b2)) / (denominator + epsilon)\n",
        "\n",
        "def extract_mtci(b6, b5, b4, epsilon=1e-8):\n",
        "    \"\"\"MERIS Terrestrial Chlorophyll Index\"\"\"\n",
        "    b6 = b6.astype(np.float32)\n",
        "    b5 = b5.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    denominator = (b5 - b4)\n",
        "    safe_denominator = np.where(np.abs(denominator) < epsilon, epsilon * np.sign(denominator + epsilon), denominator)\n",
        "    return (b6 - b5) / safe_denominator\n",
        "\n",
        "def extract_npcri(b4, b2, epsilon=1e-8):\n",
        "    \"\"\"Normalized Pigment Chlorophyll Ratio Index\"\"\"\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b2 = b2.astype(np.float32)\n",
        "    denominator = (b4 + b2)\n",
        "    return (b4 - b2) / (denominator + epsilon)\n",
        "\n",
        "def extract_mcari(b5, b4, b3, epsilon=1e-8):\n",
        "    \"\"\"Modified Chlorophyll Absorption Ratio Index\"\"\"\n",
        "    b5 = b5.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    denominator_term = (b4)\n",
        "    return ((b5 - b4) - 0.2 * (b5 - b3)) * (b5 / (denominator_term + epsilon))\n",
        "\n",
        "def extract_tcari(b5, b4, b3, epsilon=1e-8):\n",
        "    \"\"\"Transformed Chlorophyll Absorption Ratio Index\"\"\"\n",
        "    b5 = b5.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    denominator_term = (b4)\n",
        "    return 3 * ((b5 - b4) - 0.2 * (b5 - b3) * (b5 / (denominator_term + epsilon)))\n",
        "\n",
        "def extract_pvi(b8, b4):\n",
        "    \"\"\"Perpendicular Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    return (b8 - 0.3 * b4 - 0.5) / np.sqrt(1 + 0.3**2)\n",
        "\n",
        "def extract_bai(b4, b8, epsilon=1e-8):\n",
        "    \"\"\"Burn Area Index\"\"\"\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    denominator_arg = (0.1 - b4)**2 + (0.06 - b8)**2\n",
        "    # Avoid division by zero/near-zero. Also handle potential negative values if inputs are strange.\n",
        "    denominator = np.maximum(denominator_arg, epsilon) # Ensure denominator is positive and non-zero\n",
        "    return 1 / denominator\n",
        "\n",
        "\n",
        "def extract_mtvi2(b8, b3, b4):\n",
        "    \"\"\"Modified Triangular Vegetation Index 2\"\"\"\n",
        "    b8_safe = np.maximum(b8.astype(np.float32), 1e-8)\n",
        "    b3_safe = np.maximum(b3.astype(np.float32), 1e-8)\n",
        "    b4_safe = np.maximum(b4.astype(np.float32), 1e-8)\n",
        "\n",
        "    sqrt_b4 = np.sqrt(b4_safe)\n",
        "    discriminant_arg = (2 * b8_safe + 1)**2 - (6 * b8_safe - 5 * sqrt_b4) - 0.5\n",
        "    sqrt_discriminant = np.sqrt(np.maximum(discriminant_arg, 0))\n",
        "\n",
        "    mtvi2 = 1.5 * (1.2 * (b8_safe - b3_safe) - 2.5 * (b4_safe - b3_safe)) * sqrt_discriminant\n",
        "    # Set result to NaN where discriminant was negative (sqrt(negative) is not real)\n",
        "    mtvi2 = np.where(discriminant_arg < 0, np.nan, mtvi2)\n",
        "    return mtvi2\n",
        "\n",
        "def extract_ndsi(b3, b11, epsilon=1e-8):\n",
        "    \"\"\"Normalized Difference Snow Index\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b11 = b11.astype(np.float32)\n",
        "    denominator = (b3 + b11)\n",
        "    return (b3 - b11) / (denominator + epsilon)\n",
        "\n",
        "def extract_mrendvi(b6, b5, b1, epsilon=1e-8):\n",
        "    \"\"\"Modified Red Edge Normalized Difference Vegetation Index\"\"\"\n",
        "    b6 = b6.astype(np.float32)\n",
        "    b5 = b5.astype(np.float32)\n",
        "    b1 = b1.astype(np.float32)\n",
        "    denominator = (b6 + b5 - 2 * b1)\n",
        "    safe_denominator = np.where(np.abs(denominator) < epsilon, epsilon * np.sign(denominator + epsilon), denominator)\n",
        "    return (b6 - b5) / safe_denominator\n",
        "\n",
        "def extract_ndvi_re(b8, b5, epsilon=1e-8):\n",
        "    \"\"\"NDVI Red Edge\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b5 = b5.astype(np.float32)\n",
        "    denominator = (b8 + b5)\n",
        "    return (b8 - b5) / (denominator + epsilon)\n",
        "\n",
        "def extract_ci_re(b8, b5, epsilon=1e-8):\n",
        "    \"\"\"Chlorophyll Index Red Edge\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b5 = b5.astype(np.float32)\n",
        "    return (b8 / (b5 + epsilon)) - 1\n",
        "\n",
        "def extract_ndmi(b8, b11, epsilon=1e-8):\n",
        "    \"\"\"Normalized Difference Moisture Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b11 = b11.astype(np.float32)\n",
        "    denominator = (b8 + b11)\n",
        "    return (b8 - b11) / (denominator + epsilon)\n",
        "\n",
        "def extract_tndvi(b8, b4, epsilon=1e-8):\n",
        "    \"\"\"Transformed NDVI\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    ndvi_denominator = (b8 + b4)\n",
        "    ndvi = (b8 - b4) / (ndvi_denominator + epsilon)\n",
        "    arg = ndvi + 0.5\n",
        "    # Ensure sqrt argument is non-negative\n",
        "    tndvi = np.sqrt(np.maximum(arg, 0))\n",
        "    # Set result to NaN where argument was negative\n",
        "    tndvi = np.where(arg < 0, np.nan, tndvi)\n",
        "    return tndvi\n",
        "\n",
        "def extract_vdvi(b3, b4, b2, epsilon=1e-8):\n",
        "    \"\"\"Visible Difference Vegetation Index\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b2 = b2.astype(np.float32)\n",
        "    denominator = (2 * b3 + b4 + b2)\n",
        "    safe_denominator = np.where(np.abs(denominator) < epsilon, epsilon * np.sign(denominator + epsilon), denominator)\n",
        "    return (2 * b3 - b4 - b2) / safe_denominator\n",
        "\n",
        "def extract_nbr(b8, b11, epsilon=1e-8):\n",
        "    \"\"\"Normalized Burn Ratio\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b11 = b11.astype(np.float32)\n",
        "    denominator = (b8 + b11)\n",
        "    return (b8 - b11) / (denominator + epsilon)\n",
        "\n",
        "def extract_tvi(b6, b3, b4):\n",
        "    \"\"\"Triangular Vegetation Index\"\"\"\n",
        "    b6 = b6.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    return (120 * (b6 - b3) - 200 * (b4 - b3)) / 2\n",
        "\n",
        "def extract_exg(b3, b4, b2):\n",
        "    \"\"\"Excess Green Index\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b2 = b2.astype(np.float32)\n",
        "    return 2 * b3 - b4 - b2\n",
        "\n",
        "def extract_psri(b4, b2, b6, epsilon=1e-8):\n",
        "    \"\"\"Plant Senescence Reflectance Index\"\"\"\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b6 = b6.astype(np.float32)\n",
        "    # Handle division by near-zero b6\n",
        "    return (b4 - b2) / (b6 + epsilon)\n",
        "\n",
        "def extract_rdvi(b8, b4, epsilon=1e-8):\n",
        "    \"\"\"Renormalized Difference Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    arg = b8 + b4\n",
        "    # Ensure sqrt argument is non-negative\n",
        "    sqrt_sum = np.sqrt(np.maximum(arg, 0))\n",
        "    safe_sqrt_sum = np.where(np.abs(sqrt_sum) < epsilon, epsilon * np.sign(sqrt_sum + epsilon), sqrt_sum)\n",
        "    # Handle division by near-zero or zero sqrt_sum\n",
        "    rdvi = np.zeros_like(b8) # Default to zero\n",
        "    # Only calculate where safe_sqrt_sum is non-zero\n",
        "    valid_mask = safe_sqrt_sum != 0\n",
        "    rdvi[valid_mask] = (b8[valid_mask] - b4[valid_mask]) / safe_sqrt_sum[valid_mask]\n",
        "    # Set result to NaN where arg was negative initially\n",
        "    rdvi = np.where(arg < 0, np.nan, rdvi)\n",
        "    return rdvi\n",
        "\n",
        "# Ratio indices\n",
        "def extract_ratio_b1_b3(b1, b3, epsilon=1e-8):\n",
        "    b1 = b1.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    return b1 / (b3 + epsilon)\n",
        "\n",
        "def extract_ratio_b1_b5(b1, b5, epsilon=1e-8):\n",
        "    b1 = b1.astype(np.float32)\n",
        "    b5 = b5.astype(np.float32)\n",
        "    return b1 / (b5 + epsilon)\n",
        "\n",
        "def extract_ratio_b11_b12(b11, b12, epsilon=1e-8):\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b12 = b12.astype(np.float32)\n",
        "    return b11 / (b12 + epsilon)\n",
        "\n",
        "def extract_ratio_b5_b4(b5, b4, epsilon=1e-8):\n",
        "    b5 = b5.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    return b5 / (b4 + epsilon)\n",
        "\n",
        "def extract_ratio_b7_b5(b7, b5, epsilon=1e-8):\n",
        "    b7 = b7.astype(np.float32)\n",
        "    b5 = b5.astype(np.float32)\n",
        "    return b7 / (b5 + epsilon)\n",
        "\n",
        "def extract_ratio_b7_b6(b7, b6, epsilon=1e-8):\n",
        "    b7 = b7.astype(np.float32)\n",
        "    b6 = b6.astype(np.float32)\n",
        "    return b7 / (b6 + epsilon)\n",
        "\n",
        "def extract_ratio_b8_b4(b8, b4, epsilon=1e-8):\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    return b8 / (b4 + epsilon)\n",
        "\n",
        "# Additional advanced indices\n",
        "def extract_lai(b8, b4, epsilon=1e-8):\n",
        "    \"\"\"Leaf Area Index approximation using NDVI\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    ndvi_denominator = (b8 + b4)\n",
        "    ndvi = (b8 - b4) / (ndvi_denominator + epsilon)\n",
        "    ndvi_clipped = np.clip(ndvi, -1.0, 0.689) # Clip to range\n",
        "    numerator = 0.69 - ndvi_clipped\n",
        "    denominator = 0.59 + ndvi_clipped\n",
        "    # Avoid division by zero/near-zero and ensure log argument is positive\n",
        "    ratio = np.maximum(numerator / (denominator + epsilon), epsilon)\n",
        "    log_ratio = np.log(ratio)\n",
        "    # Avoid division by zero\n",
        "    safe_divisor = -0.91 # Constant\n",
        "    lai = log_ratio / (safe_divisor + epsilon if np.abs(safe_divisor) < epsilon else safe_divisor) # Added epsilon for safety if divisor was variable\n",
        "\n",
        "    # Ensure result is finite (log(<=0) is inf/nan)\n",
        "    lai = np.where(np.isfinite(lai), lai, np.nan)\n",
        "    return lai\n",
        "\n",
        "def extract_cab(b5, b4, epsilon=1e-8):\n",
        "    \"\"\"Chlorophyll a+b concentration approximation\"\"\"\n",
        "    b5 = b5.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    denominator = (b5 + b4)\n",
        "    return 100 * (b5 - b4) / (denominator + epsilon)\n",
        "\n",
        "def extract_car(b2, b3, b4, epsilon=1e-8):\n",
        "    \"\"\"Carotenoid Reflectance Index\"\"\"\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    # Avoid division by zero/near-zero for reciprocals\n",
        "    recip_b2 = 1 / (b2 + epsilon)\n",
        "    recip_b3 = 1 / (b3 + epsilon)\n",
        "    return (recip_b2 - recip_b3) * b4\n",
        "\n",
        "def extract_cri1(b2, b5, epsilon=1e-8):\n",
        "    \"\"\"Carotenoid Reflectance Index 1\"\"\"\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b5 = b5.astype(np.float32)\n",
        "    # Avoid division by zero/near-zero for reciprocals\n",
        "    recip_b2 = 1 / (b2 + epsilon)\n",
        "    recip_b5 = 1 / (b5 + epsilon)\n",
        "    return recip_b2 - recip_b5\n",
        "\n",
        "def extract_cri2(b2, b7, epsilon=1e-8):\n",
        "    \"\"\"Carotenoid Reflectance Index 2\"\"\"\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b7 = b7.astype(np.float32)\n",
        "    # Avoid division by zero/near-zero for reciprocals\n",
        "    recip_b2 = 1 / (b2 + epsilon)\n",
        "    recip_b7 = 1 / (b7 + epsilon)\n",
        "    return recip_b2 - recip_b7\n",
        "\n",
        "def extract_pri(b2, b3, epsilon=1e-8):\n",
        "    \"\"\"Photochemical Reflectance Index\"\"\"\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    denominator = (b2 + b3)\n",
        "    return (b2 - b3) / (denominator + epsilon)\n",
        "\n",
        "def extract_lwi(b8, b11, epsilon=1e-8):\n",
        "    \"\"\"Leaf Water Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b11 = b11.astype(np.float32)\n",
        "    denominator = (b8 + b11)\n",
        "    return (b8 - b11) / (denominator + epsilon)\n",
        "\n",
        "def extract_msi(b11, b8, epsilon=1e-8):\n",
        "    \"\"\"Moisture Stress Index\"\"\"\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    return b11 / (b8 + epsilon)\n",
        "\n",
        "def extract_wi(b8, b3, epsilon=1e-8):\n",
        "    \"\"\"Water Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    return b8 / (b3 + epsilon)\n",
        "\n",
        "def extract_fe2(b4, b8, epsilon=1e-8):\n",
        "    \"\"\"Iron Oxide Ratio\"\"\"\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    return b4 / (b8 + epsilon)\n",
        "\n",
        "def extract_fe3(b2, b4, epsilon=1e-8):\n",
        "    \"\"\"Iron Oxide Ratio 2\"\"\"\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    return b2 / (b4 + epsilon)\n",
        "\n",
        "def extract_clay(b11, b12, epsilon=1e-8):\n",
        "    \"\"\"Clay Minerals Ratio\"\"\"\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b12 = b12.astype(np.float32)\n",
        "    return b11 / (b12 + epsilon)\n",
        "\n",
        "# NEW ADDITIONAL INDICES\n",
        "\n",
        "# Advanced Vegetation Indices\n",
        "def extract_atsavi(b8, b4, L=0.08, epsilon=1e-8):\n",
        "    \"\"\"Adjusted Transformed Soil-Adjusted Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    numerator = 0.15 * (b8 - 0.15 * b4 - 0.08)\n",
        "    denominator = (b8 + 0.15 * b4 + 0.08)\n",
        "    return numerator / (denominator + epsilon)\n",
        "\n",
        "def extract_gemi(b8, b4, epsilon=1e-8):\n",
        "    \"\"\"Global Environment Monitoring Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    denominator_eta = (b8 + b4 + 0.5)\n",
        "    # Avoid division by zero/near-zero for eta denominator\n",
        "    eta = (2 * (b8**2 - b4**2) + 1.5 * b8 + 0.5 * b4) / (denominator_eta + epsilon)\n",
        "    denominator_term2 = (1 - b4)\n",
        "    safe_denominator_term2 = np.where(np.abs(denominator_term2) < epsilon, epsilon * np.sign(denominator_term2 + epsilon), denominator_term2)\n",
        "    term2 = (b4 - 0.125) / safe_denominator_term2\n",
        "    return eta * (1 - 0.25 * eta) - term2\n",
        "\n",
        "def extract_ipvi(b8, b4, epsilon=1e-8):\n",
        "    \"\"\"Infrared Percentage Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    denominator = (b8 + b4)\n",
        "    return b8 / (denominator + epsilon)\n",
        "\n",
        "def extract_dvi(b8, b4):\n",
        "    \"\"\"Difference Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    return b8 - b4\n",
        "\n",
        "def extract_rvi(b8, b4, epsilon=1e-8):\n",
        "    \"\"\"Ratio Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    return b8 / (b4 + epsilon)\n",
        "\n",
        "def extract_sr(b8, b4, epsilon=1e-8):\n",
        "    \"\"\"Simple Ratio\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    return b8 / (b4 + epsilon)\n",
        "\n",
        "def extract_tsavi(b8, b4, L=0.08, s=0.15, epsilon=1e-8):\n",
        "    \"\"\"Transformed Soil Adjusted Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    # Using user's formula structure\n",
        "    numerator_user = s * (b8 - s * b4 - L)\n",
        "    denominator_user = (s * b8 + b4 - s * L)\n",
        "    return numerator_user / (denominator_user + epsilon)\n",
        "\n",
        "def extract_msavi(b8, b4, epsilon=1e-8):\n",
        "    \"\"\"Modified Soil Adjusted Vegetation Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    arg_sqrt = (2 * b8 + 1)**2 - 8 * (b8 - b4)\n",
        "    # Ensure sqrt argument is non-negative\n",
        "    safe_arg_sqrt = np.maximum(arg_sqrt, 0)\n",
        "    msavi = (2 * b8 + 1 - np.sqrt(safe_arg_sqrt)) / 2\n",
        "    # Set result to NaN where arg_sqrt was negative\n",
        "    msavi = np.where(arg_sqrt < 0, np.nan, msavi)\n",
        "    return msavi\n",
        "\n",
        "def extract_msavi2(b8, b4):\n",
        "    \"\"\"Modified Soil Adjusted Vegetation Index 2\"\"\"\n",
        "    # Note: User's formula is identical to MSAVI. Re-using MSAVI implementation.\n",
        "    # If MSAVI2 formula is different, this function should be updated.\n",
        "    return extract_msavi(b8, b4) # Calls the extract_msavi function\n",
        "\n",
        "def extract_gari(b8, b3, b4, b2, gamma=1.0, epsilon=1e-8):\n",
        "    \"\"\"Green Atmospherically Resistant Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b2 = b2.astype(np.float32)\n",
        "    term = b3 - (b2 - b4)\n",
        "    denominator = (b8 + term)\n",
        "    return (b8 - term) / (denominator + epsilon)\n",
        "\n",
        "def extract_gli(b3, b4, b2, epsilon=1e-8):\n",
        "    \"\"\"Green Leaf Index\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b2 = b2.astype(np.float32)\n",
        "    denominator = (2 * b3 + b4 + b2)\n",
        "    return (2 * b3 - b4 - b2) / (denominator + epsilon)\n",
        "\n",
        "def extract_nli(b8, b4, epsilon=1e-8):\n",
        "    \"\"\"Non-Linear Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    denominator = (b8**2 + b4)\n",
        "    return (b8**2 - b4) / (denominator + epsilon)\n",
        "\n",
        "# Water Indices\n",
        "def extract_awei_ns(b3, b11, b8, b12): # Linear combination\n",
        "    \"\"\"Automated Water Extraction Index (No Shadow)\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b12 = b12.astype(np.float32)\n",
        "    return 4 * (b3 - b11) - 0.25 * b8 + 2.75 * b12\n",
        "\n",
        "def extract_awei_sh(b2, b3, b11, b8, b12): # Linear combination\n",
        "    \"\"\"Automated Water Extraction Index (with Shadow)\"\"\"\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b12 = b12.astype(np.float32)\n",
        "    return b2 + 2.5 * b3 - 1.5 * (b8 + b11) - 0.25 * b12\n",
        "\n",
        "def extract_wi2015(b3, b8, b11, b12): # Linear combination\n",
        "    \"\"\"Water Index 2015\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b12 = b12.astype(np.float32)\n",
        "    return 1.7204 + 171 * b3 + 3 * b8 - 70 * b11 - 45 * b12\n",
        "\n",
        "def extract_tcw(b2, b3, b4, b8, b11, b12): # Linear combination\n",
        "    \"\"\"Tasseled Cap Wetness\"\"\"\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b12 = b12.astype(np.float32)\n",
        "    return 0.0315 * b2 + 0.2021 * b3 + 0.3102 * b4 + 0.1595 * b8 - 0.6806 * b11 - 0.6109 * b12\n",
        "\n",
        "def extract_tcb(b2, b3, b4, b8, b11, b12): # Linear combination\n",
        "    \"\"\"Tasseled Cap Brightness\"\"\"\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b12 = b12.astype(np.float32)\n",
        "    return 0.3037 * b2 + 0.2793 * b3 + 0.4743 * b4 + 0.5585 * b8 + 0.5082 * b11 + 0.1863 * b12\n",
        "\n",
        "def extract_tcg(b2, b3, b4, b8, b11, b12): # Linear combination\n",
        "    \"\"\"Tasseled Cap Greenness\"\"\"\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b12 = b12.astype(np.float32)\n",
        "    return -0.2848 * b2 - 0.2435 * b3 - 0.5436 * b4 + 0.7243 * b8 + 0.0840 * b11 - 0.1800 * b12\n",
        "\n",
        "# Geological and Soil Indices\n",
        "def extract_ndti(b11, b12, epsilon=1e-8):\n",
        "    \"\"\"Normalized Difference Tillage Index\"\"\"\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b12 = b12.astype(np.float32)\n",
        "    denominator = (b11 + b12)\n",
        "    return (b11 - b12) / (denominator + epsilon)\n",
        "\n",
        "def extract_sti(b11, b12, epsilon=1e-8):\n",
        "    \"\"\"Soil Tillage Index\"\"\"\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b12 = b12.astype(np.float32)\n",
        "    return b11 / (b12 + epsilon)\n",
        "\n",
        "def extract_ci(b4, b3, epsilon=1e-8):\n",
        "    \"\"\"Coloration Index\"\"\"\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    denominator = (b4 + b3)\n",
        "    return (b4 - b3) / (denominator + epsilon)\n",
        "\n",
        "def extract_hue(b2, b3, b4, epsilon=1e-8):\n",
        "    \"\"\"Hue Index\"\"\"\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    denominator = (30.5 * (b3 - b2))\n",
        "    safe_denominator = np.where(np.abs(denominator) < epsilon, epsilon * np.sign(denominator + epsilon), denominator)\n",
        "    arg = 2 * (b4 - b3 - b2) / safe_denominator\n",
        "    # Avoid log/atanh of non-finite values if arg is inf/nan\n",
        "    arg = np.clip(arg, -1e10, 1e10) # Simple clipping to prevent inf, although arctan handles inf\n",
        "    return np.arctan(arg)\n",
        "\n",
        "def extract_sat(b2, b3, b4, epsilon=1e-8):\n",
        "    \"\"\"Saturation Index\"\"\"\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    max_band = np.maximum(np.maximum(b2, b3), b4)\n",
        "    min_band = np.minimum(np.minimum(b2, b3), b4)\n",
        "    safe_denominator = max_band + epsilon # Avoid division by zero max_band\n",
        "    return (max_band - min_band) / safe_denominator\n",
        "\n",
        "def extract_int(b2, b3, b4):\n",
        "    \"\"\"Intensity Index\"\"\"\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    return (b2 + b3 + b4) / 3.0\n",
        "\n",
        "# Urban and Built-up Indices\n",
        "def extract_ndbi(b11, b8, epsilon=1e-8):\n",
        "    \"\"\"Normalized Difference Built-up Index\"\"\"\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    denominator = (b11 + b8)\n",
        "    return (b11 - b8) / (denominator + epsilon)\n",
        "\n",
        "def extract_ebbi(b11, b8, b4, epsilon=1e-8):\n",
        "    \"\"\"Enhanced Built-Up and Bareness Index\"\"\"\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    numerator = (b11 - b8)\n",
        "    denominator_sqrt_arg = b11 + b4\n",
        "    # Ensure sqrt argument is non-negative\n",
        "    safe_denominator_sqrt_arg = np.maximum(denominator_sqrt_arg, 0)\n",
        "    denominator = 10 * np.sqrt(safe_denominator_sqrt_arg)\n",
        "    # Avoid division by zero/near-zero\n",
        "    safe_denominator = np.where(np.abs(denominator) < epsilon, epsilon * np.sign(denominator + epsilon), denominator)\n",
        "    ebbi = np.zeros_like(b11) # Default to zero\n",
        "    valid_mask = safe_denominator != 0\n",
        "    ebbi[valid_mask] = numerator[valid_mask] / safe_denominator[valid_mask]\n",
        "    # Set result to NaN where arg was negative initially\n",
        "    ebbi = np.where(denominator_sqrt_arg < 0, np.nan, ebbi)\n",
        "    return ebbi\n",
        "\n",
        "\n",
        "def extract_ui(b11, b8, epsilon=1e-8):\n",
        "    \"\"\"Urban Index\"\"\"\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    denominator = (b11 + b8)\n",
        "    return (b11 - b8) / (denominator + epsilon)\n",
        "\n",
        "def extract_ibi(b11, b8, b3, b4, epsilon=1e-8):\n",
        "    \"\"\"Index-based Built-up Index\"\"\"\n",
        "    b11 = b11.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    ndbi_denominator = (b11 + b8)\n",
        "    ndbi = (b11 - b8) / (ndbi_denominator + epsilon)\n",
        "\n",
        "    savi_comp_denominator = (b8 + b4 + 0.5)\n",
        "    savi_comp = ((b8 - b4) * 1.5) / (savi_comp_denominator + epsilon)\n",
        "\n",
        "    mndwi_denominator = (b3 + b11)\n",
        "    mndwi = (b3 - b11) / (mndwi_denominator + epsilon)\n",
        "\n",
        "    avg_comp = (savi_comp + mndwi) / 2.0 # This might still be NaN if savi_comp or mndwi is NaN\n",
        "    avg_comp = np.nan_to_num(avg_comp, nan=0.0) # Replace NaN with 0 for the average step safety\n",
        "\n",
        "    numerator = (ndbi - avg_comp)\n",
        "    denominator = (ndbi + avg_comp)\n",
        "    # Avoid division by zero/near-zero for final IBI denominator\n",
        "    return numerator / (denominator + epsilon)\n",
        "\n",
        "# Snow and Ice Indices\n",
        "def extract_s3(b3, b4, b8, epsilon=1e-8):\n",
        "    \"\"\"S3 Snow Index\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b8 = b8.astype(np.float32)\n",
        "    numerator = b8 * (b4 - b3)\n",
        "    denominator = (b8 + b4) * (b8 + b3)\n",
        "    # Avoid division by zero/near-zero\n",
        "    return numerator / (denominator + epsilon)\n",
        "\n",
        "def extract_ndsii(b3, b4, epsilon=1e-8):\n",
        "    \"\"\"Normalized Difference Snow and Ice Index\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    denominator = (b3 + b4)\n",
        "    return (b3 - b4) / (denominator + epsilon)\n",
        "\n",
        "def extract_snowmap(b2, b11, epsilon=1e-8):\n",
        "    \"\"\"Snow Mapping Index\"\"\"\n",
        "    b2 = b2.astype(np.float32)\n",
        "    b11 = b11.astype(np.float32)\n",
        "    denominator = (b2 + b11)\n",
        "    return (b2 - b11) / (denominator + epsilon)\n",
        "\n",
        "# Atmospheric Indices\n",
        "def extract_vari(b3, b4, b2, epsilon=1e-8):\n",
        "    \"\"\"Visible Atmospherically Resistant Index\"\"\"\n",
        "    b3 = b3.astype(np.float32)\n",
        "    b4 = b4.astype(np.float32)\n",
        "    b2 = b2.astype(np.float32)\n",
        "    denominator = (b3 + b4 - b2)\n",
        "    return (b3 - b4) / (denominator + epsilon)\n",
        "\n",
        "def extract_gvmi(b8, b11, epsilon=1e-8):\n",
        "    \"\"\"Global Vegetation Moisture Index\"\"\"\n",
        "    b8 = b8.astype(np.float32)\n",
        "    b11 = b11.astype(np.float32)\n",
        "    numerator = ((b8 + 0.1) - (b11 + 0.02))\n",
        "    denominator = ((b8 + 0.1) + (b11 + 0.02))\n",
        "    return numerator / (denominator + epsilon)\n",
        "\n",
        "# Texture Features\n",
        "def extract_glcm_features(band, distances=[1], angles=[0, 45, 90, 135]):\n",
        "    \"\"\"Extract Gray-Level Co-occurrence Matrix features\"\"\"\n",
        "    features = []\n",
        "    glcm_props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
        "    expected_features_count = len(distances) * len(angles) * len(glcm_props)\n",
        "\n",
        "    try:\n",
        "        # Handle potential NaNs/Inf before normalization\n",
        "        band_clean = np.nan_to_num(band.astype(np.float32), nan=np.nanmedian(band) if np.nanmedian(band) is not np.nan else 0.0)\n",
        "        band_clean = np.clip(band_clean, 0, 65535) # Clip to a reasonable 16-bit range as uint8 is target\n",
        "\n",
        "        min_val = np.nanmin(band_clean)\n",
        "        max_val = np.nanmax(band_clean)\n",
        "\n",
        "        # Normalize to uint8 [0, 255]\n",
        "        if max_val - min_val < 1e-8:\n",
        "             band_norm = np.zeros(band_clean.shape, dtype=np.uint8)\n",
        "        else:\n",
        "             # Scale to 0-255 and convert to uint8\n",
        "             band_norm = ((band_clean - min_val) / (max_val - min_val) * 255.0).astype(np.uint8)\n",
        "\n",
        "        levels = 256 # Using 256 levels after converting to uint8\n",
        "\n",
        "        for distance in distances:\n",
        "            for angle in angles:\n",
        "                try:\n",
        "                    # Check if image is large enough for the distance\n",
        "                    if band_norm.shape[0] <= distance or band_norm.shape[1] <= distance:\n",
        "                         features.extend([0.0] * len(glcm_props))\n",
        "                         continue\n",
        "\n",
        "                    # Check if all values are the same after normalization (avoids GLCM error)\n",
        "                    if np.min(band_norm) == np.max(band_norm):\n",
        "                         features.extend([0.0] * len(glcm_props))\n",
        "                         continue\n",
        "\n",
        "                    # GLCM computation might raise warnings or errors on edge cases or weird inputs\n",
        "                    try:\n",
        "                        glcm = graycomatrix(band_norm, [distance], [np.radians(angle)],\n",
        "                                         levels=levels, symmetric=True, normed=True)\n",
        "\n",
        "                        if glcm.size > 0:\n",
        "                             # Ensure glcm has the expected shape before accessing elements\n",
        "                             if glcm.shape == (levels, levels, 1, 1):\n",
        "                                contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
        "                                dissimilarity = graycoprops(glcm, 'dissimilarity')[0, 0]\n",
        "                                homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]\n",
        "                                energy = graycoprops(glcm, 'energy')[0, 0]\n",
        "                                correlation = graycoprops(glcm, 'correlation')[0, 0]\n",
        "                                # Ensure finite values, replace with 0 if nan/inf\n",
        "                                features.extend([\n",
        "                                    np.nan_to_num(contrast, nan=0.0, posinf=0.0, neginf=0.0),\n",
        "                                    np.nan_to_num(dissimilarity, nan=0.0, posinf=0.0, neginf=0.0),\n",
        "                                    np.nan_to_num(homogeneity, nan=0.0, posinf=0.0, neginf=0.0),\n",
        "                                    np.nan_to_num(energy, nan=0.0, posinf=0.0, neginf=0.0),\n",
        "                                    np.nan_to_num(correlation, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                                ])\n",
        "                             else:\n",
        "                                # Unexpected GLCM shape\n",
        "                                features.extend([0.0] * len(glcm_props))\n",
        "\n",
        "                        else:\n",
        "                             # GLCM was empty (can happen for tiny images or certain inputs)\n",
        "                             features.extend([0.0] * len(glcm_props))\n",
        "\n",
        "                    except Exception as glcm_e:\n",
        "                         # Catch errors specifically from graycomatrix/graycoprops\n",
        "                         features.extend([0.0] * len(glcm_props))\n",
        "                         # print(f\"Error in GLCM for distance {distance}, angle {angle}: {glcm_e}\") # Avoid printing\n",
        "\n",
        "                except Exception as angle_e:\n",
        "                    # Error during processing for a specific angle/distance iteration (less likely now)\n",
        "                    features.extend([0.0] * len(glcm_props))\n",
        "                    # print(f\"Unexpected error in GLCM angle loop for distance {distance}, angle {angle}: {angle_e}\") # Avoid printing\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        # General error during GLCM extraction setup or iteration\n",
        "        features = [0.0] * expected_features_count\n",
        "        # print(f\"General error extracting GLCM features: {e}\") # Avoid printing in parallel job\n",
        "\n",
        "    # Ensure the correct number of features is returned even if errors occurred mid-loop\n",
        "    if len(features) != expected_features_count:\n",
        "         features = features[:expected_features_count] + [0.0] * (expected_features_count - len(features))\n",
        "\n",
        "    return features\n",
        "\n",
        "def extract_gabor_features(band, frequencies=[0.1, 0.3, 0.5]):\n",
        "    \"\"\"Extract Gabor filter features\"\"\"\n",
        "    features = []\n",
        "    gabor_stats = ['mean', 'std', 'max', 'min']\n",
        "    expected_features_count = len(frequencies) * len(gabor_stats)\n",
        "\n",
        "    try:\n",
        "        band_float = band.astype(np.float32)\n",
        "        band_clean = np.nan_to_num(band_float, nan=np.nanmean(band_float) if np.nanmean(band_float) is not np.nan else 0.0)\n",
        "\n",
        "        for freq in frequencies:\n",
        "            try:\n",
        "                min_size_heuristic = 33 # Heuristic minimum size for Gabor kernel\n",
        "\n",
        "                if band_clean.shape[0] < min_size_heuristic or band_clean.shape[1] < min_size_heuristic:\n",
        "                     features.extend([0.0] * len(gabor_stats))\n",
        "                     continue\n",
        "\n",
        "                # Compute magnitude response\n",
        "                # Gabor computation might raise errors for small sizes or specific inputs\n",
        "                try:\n",
        "                     real, imag = gabor(band_clean, frequency=freq)\n",
        "                     magnitude = np.sqrt(real**2 + imag**2) # Magnitude response\n",
        "\n",
        "                     # Calculate stats on the magnitude image\n",
        "                     mean_val = np.nanmean(magnitude)\n",
        "                     std_val = np.nanstd(magnitude)\n",
        "                     max_val = np.nanmax(magnitude)\n",
        "                     min_val = np.nanmin(magnitude)\n",
        "\n",
        "                     # Ensure finite values, replace with 0 if nan/inf\n",
        "                     features.extend([\n",
        "                         np.nan_to_num(mean_val, nan=0.0, posinf=0.0, neginf=0.0),\n",
        "                         np.nan_to_num(std_val, nan=0.0, posinf=0.0, neginf=0.0),\n",
        "                         np.nan_to_num(max_val, nan=0.0, posinf=0.0, neginf=0.0),\n",
        "                         np.nan_to_num(min_val, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                     ])\n",
        "                except Exception as gabor_e:\n",
        "                     # Catch errors specifically from gabor or stats calculation\n",
        "                     features.extend([0.0] * len(gabor_stats))\n",
        "                     # print(f\"Error in Gabor for freq {freq}: {gabor_e}\") # Avoid printing\n",
        "\n",
        "            except Exception as freq_e:\n",
        "                # Error processing a specific frequency iteration (less likely now)\n",
        "                features.extend([0.0] * len(gabor_stats))\n",
        "                # print(f\"Unexpected error in Gabor frequency loop for freq {freq}: {freq_e}\") # Avoid printing\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        # General error during Gabor extraction setup or iteration\n",
        "        features = [0.0] * expected_features_count\n",
        "        # print(f\"General error extracting Gabor features: {e}\") # Avoid printing in parallel job\n",
        "\n",
        "    if len(features) != expected_features_count:\n",
        "         features = features[:expected_features_count] + [0.0] * (expected_features_count - len(features))\n",
        "\n",
        "    # Ensure all features are finite (redundant with nan_to_num inside, but harmless)\n",
        "    return [np.nan_to_num(f, nan=0.0, posinf=0.0, neginf=0.0) for f in features]\n",
        "\n",
        "def extract_local_binary_pattern(band, P=8, R=1):\n",
        "    \"\"\"Extract Local Binary Pattern features (histogram of uniform patterns)\"\"\"\n",
        "    features = []\n",
        "    n_bins = P + 2 # Number of bins for uniform LBP (P neighbors) + 2 (0 and P+1)\n",
        "    expected_features_count = n_bins\n",
        "\n",
        "    try:\n",
        "        band_float = band.astype(np.float32)\n",
        "        band_clean = np.nan_to_num(band_float, nan=np.nanmean(band_float) if np.nanmean(band_float) is not np.nan else 0.0)\n",
        "\n",
        "        min_size = int(2*R + 1)\n",
        "        if band_clean.shape[0] < min_size or band_clean.shape[1] < min_size:\n",
        "             return [0.0] * expected_features_count\n",
        "\n",
        "        min_val = np.nanmin(band_clean)\n",
        "        max_val = np.nanmax(band_clean)\n",
        "        if max_val - min_val < 1e-8:\n",
        "             band_norm_uint8 = np.zeros(band_clean.shape, dtype=np.uint8)\n",
        "        else:\n",
        "             band_norm_uint8 = ((band_clean - min_val) / (max_val - min_val) * 255.0).astype(np.uint8)\n",
        "\n",
        "        # Compute LBP\n",
        "        # LBP computation might raise errors on edge cases\n",
        "        try:\n",
        "            lbp_image = local_binary_pattern(band_norm_uint8, P=P, R=R, method='uniform')\n",
        "\n",
        "            # Compute histogram of LBP codes\n",
        "            # Histogram should be safe after LBP computation\n",
        "            hist, _ = np.histogram(lbp_image, bins=n_bins, range=(0, n_bins), density=True) # density=True gives probabilities/normalized counts\n",
        "\n",
        "            features.extend(hist.tolist())\n",
        "        except Exception as lbp_e:\n",
        "            # Catch errors specifically from local_binary_pattern or histogram\n",
        "            features = [0.0] * expected_features_count\n",
        "            # print(f\"Error in LBP R={R}: {lbp_e}\") # Avoid printing\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        # General error during LBP extraction setup\n",
        "        features = [0.0] * expected_features_count\n",
        "        # print(f\"General error extracting LBP features: {e}\") # Avoid printing in parallel job\n",
        "\n",
        "\n",
        "    if len(features) != expected_features_count:\n",
        "         features = features[:expected_features_count] + [0.0] * (expected_features_count - len(features))\n",
        "\n",
        "    # Ensure features are finite\n",
        "    return [np.nan_to_num(f, nan=0.0, posinf=0.0, neginf=0.0) for f in features]\n",
        "\n",
        "def extract_morphological_features(band):\n",
        "    \"\"\"Extract morphological features based on binary thresholding\"\"\"\n",
        "    features = []\n",
        "    morph_props = ['OpeningRatio', 'ClosingRatio', 'ErosionRatio', 'DilationRatio']\n",
        "    expected_features_count = len(morph_props)\n",
        "\n",
        "    try:\n",
        "        band_float = band.astype(np.float32)\n",
        "        band_clean = np.nan_to_num(band_float, nan=np.nanmedian(band_float) if np.nanmedian(band_float) is not np.nan else 0.0)\n",
        "\n",
        "        mean_val = np.nanmean(band_clean)\n",
        "        if np.isnan(mean_val):\n",
        "             return [0.0] * expected_features_count\n",
        "\n",
        "        thresh = band_clean > mean_val # Boolean array\n",
        "\n",
        "        structure = np.ones((3, 3), dtype=bool)\n",
        "\n",
        "        if band_clean.shape[0] < 3 or band_clean.shape[1] < 3:\n",
        "             return [0.0] * expected_features_count\n",
        "\n",
        "        # Morphological operations might raise errors on edge cases\n",
        "        try:\n",
        "            opened = binary_opening(thresh, structure=structure)\n",
        "            closed = binary_closing(thresh, structure=structure)\n",
        "            eroded = binary_erosion(thresh, structure=structure)\n",
        "            dilated = binary_dilation(thresh, structure=structure)\n",
        "\n",
        "            total_thresh_area = np.sum(thresh)\n",
        "\n",
        "            if total_thresh_area > 0:\n",
        "                 features = [\n",
        "                    np.sum(opened) / total_thresh_area,\n",
        "                    np.sum(closed) / total_thresh_area,\n",
        "                    np.sum(eroded) / total_thresh_area,\n",
        "                    np.sum(dilated) / total_thresh_area\n",
        "                ]\n",
        "            else:\n",
        "                 features = [0.0] * expected_features_count\n",
        "        except Exception as morph_e:\n",
        "            # Catch errors from binary morphology\n",
        "            features = [0.0] * expected_features_count\n",
        "            # print(f\"Error in morphology: {morph_e}\") # Avoid printing\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        # General error during morphological feature extraction setup\n",
        "        features = [0.0] * expected_features_count\n",
        "        # print(f\"General error extracting morphological features: {e}\") # Avoid printing in parallel job\n",
        "\n",
        "\n",
        "    if len(features) != expected_features_count:\n",
        "         features = features[:expected_features_count] + [0.0] * (expected_features_count - len(features))\n",
        "\n",
        "    # Ensure features are finite\n",
        "    return [np.nan_to_num(f, nan=0.0, posinf=0.0, neginf=0.0) for f in features]\n",
        "\n",
        "\n",
        "def extract_entropy_features(band):\n",
        "    \"\"\"Extract entropy-based features (Shannon and local entropy stats)\"\"\"\n",
        "    features = []\n",
        "    entropy_stats = ['Shannon', 'LocalMean', 'LocalStd']\n",
        "    expected_features_count = len(entropy_stats)\n",
        "\n",
        "    try:\n",
        "        band_float = band.astype(np.float32)\n",
        "        band_clean = np.nan_to_num(band_float, nan=np.nanmedian(band_float) if np.nanmedian(band_float) is not np.nan else 0.0)\n",
        "        band_clean = np.clip(band_clean, 0, 65535) # Clip to a reasonable 16-bit range as uint8 is target\n",
        "\n",
        "\n",
        "        min_val = np.nanmin(band_clean)\n",
        "        max_val = np.nanmax(band_clean)\n",
        "        if max_val - min_val < 1e-8:\n",
        "             band_norm_uint8 = np.zeros(band_clean.shape, dtype=np.uint8)\n",
        "        else:\n",
        "             band_norm_uint8 = ((band_clean - min_val) / (max_val - min_val) * 255.0).astype(np.uint8)\n",
        "\n",
        "\n",
        "        # Calculate global Shannon entropy\n",
        "        unique_vals = np.unique(band_norm_uint8)\n",
        "        shannon_ent = 0.0 # Default\n",
        "        if len(unique_vals) > 1: # Entropy is 0 if only one value is present\n",
        "             # Skimage shannon_entropy works on array directly\n",
        "             try:\n",
        "                 shannon_ent = shannon_entropy(band_norm_uint8)\n",
        "             except Exception as se_e:\n",
        "                 shannon_ent = 0.0 # Handle errors\n",
        "\n",
        "\n",
        "        # Calculate local entropy using skimage.filters.rank.entropy\n",
        "        radius = 3 # Radius of the local neighborhood\n",
        "        selem = disk(radius) # Structuring element for local entropy\n",
        "\n",
        "        local_ent_mean = 0.0\n",
        "        local_ent_std = 0.0\n",
        "\n",
        "        if band_norm_uint8.shape[0] >= selem.shape[0] and band_norm_uint8.shape[1] >= selem.shape[1]:\n",
        "            try:\n",
        "                # skimage.filters.rank.entropy requires uint8 or uint16 input\n",
        "                local_ent = entropy(band_norm_uint8, selem=selem)\n",
        "                # Calculate stats (mean, std) on the local entropy map\n",
        "                local_ent_mean = np.nanmean(local_ent)\n",
        "                local_ent_std = np.nanstd(local_ent)\n",
        "            except Exception as lre_e:\n",
        "                 # Handle errors from rank entropy or stats\n",
        "                 local_ent_mean = 0.0\n",
        "                 local_ent_std = 0.0\n",
        "                 # print(f\"Error in local rank entropy radius {radius}: {lre_e}\") # Avoid printing\n",
        "\n",
        "\n",
        "        features = [shannon_ent, local_ent_mean, local_ent_std]\n",
        "\n",
        "    except Exception as e:\n",
        "        # General error during entropy extraction setup\n",
        "        features = [0.0] * expected_features_count\n",
        "        # print(f\"General error extracting entropy features: {e}\") # Avoid printing in parallel job\n",
        "\n",
        "\n",
        "    if len(features) != expected_features_count:\n",
        "         features = features[:expected_features_count] + [0.0] * (expected_features_count - len(features))\n",
        "\n",
        "    # Ensure features are finite\n",
        "    return [np.nan_to_num(f, nan=0.0, posinf=0.0, neginf=0.0) for f in features]\n",
        "\n",
        "\n",
        "def extract_edge_features(band):\n",
        "    \"\"\"Extract edge detection features (sum, mean, std of edge maps)\"\"\"\n",
        "    features = []\n",
        "    edge_types = ['canny', 'sobel', 'prewitt', 'roberts']\n",
        "    edge_stats = ['sum', 'mean', 'std']\n",
        "    expected_features_count = len(edge_types) * len(edge_stats)\n",
        "\n",
        "    try:\n",
        "        band_float = band.astype(np.float32)\n",
        "\n",
        "        min_size = 3 # Minimum size for 3x3 kernel\n",
        "        if band_float.shape[0] < min_size or band_float.shape[1] < min_size:\n",
        "             return [0.0] * expected_features_count\n",
        "\n",
        "        band_clean = np.nan_to_num(band_float, nan=np.nanmedian(band_float) if np.nanmedian(band_float) is not np.nan else 0.0)\n",
        "        # Optional pre-smoothing can be added here if needed, but might affect performance\n",
        "\n",
        "        # Apply edge detectors and calculate stats, handling potential errors for each\n",
        "        edge_maps = {}\n",
        "        try:\n",
        "            edge_maps['canny'] = canny(band_clean).astype(np.float32)\n",
        "        except Exception:\n",
        "            edge_maps['canny'] = np.zeros_like(band_clean, dtype=np.float32)\n",
        "\n",
        "        try:\n",
        "            edge_maps['sobel'] = sobel(band_clean)\n",
        "        except Exception:\n",
        "            edge_maps['sobel'] = np.zeros_like(band_clean, dtype=np.float32)\n",
        "\n",
        "        try:\n",
        "            edge_maps['prewitt'] = prewitt(band_clean)\n",
        "        except Exception:\n",
        "            edge_maps['prewitt'] = np.zeros_like(band_clean, dtype=np.float32)\n",
        "\n",
        "        try:\n",
        "            edge_maps['roberts'] = roberts(band_clean)\n",
        "        except Exception:\n",
        "            edge_maps['roberts'] = np.zeros_like(band_clean, dtype=np.float32)\n",
        "\n",
        "        # Calculate stats for each edge map\n",
        "        def get_stats(edge_map):\n",
        "            s = np.nansum(edge_map)\n",
        "            m = np.nanmean(edge_map)\n",
        "            sd = np.nanstd(edge_map)\n",
        "            # Ensure finite values\n",
        "            return [np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0) for x in [s, m, sd]]\n",
        "\n",
        "        # Append stats in the defined order\n",
        "        for edge_type in edge_types:\n",
        "             features.extend(get_stats(edge_maps.get(edge_type, np.zeros_like(band_clean, dtype=np.float32))))\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        # General error during edge feature extraction setup\n",
        "        features = [0.0] * expected_features_count\n",
        "        # print(f\"General error extracting edge features: {e}\") # Avoid printing in parallel job\n",
        "\n",
        "\n",
        "    if len(features) != expected_features_count:\n",
        "         features = features[:expected_features_count] + [0.0] * (expected_features_count - len(features))\n",
        "\n",
        "    # Ensure features are finite\n",
        "    return [np.nan_to_num(f, nan=0.0, posinf=0.0, neginf=0.0) for f in features]\n",
        "\n",
        "\n",
        "# Helper function for batch stats\n",
        "def batch_stats(arr_batch):\n",
        "    \"\"\"Computes mean, median, std, min, max for a batch of 2D arrays (N, H, W).\"\"\"\n",
        "    # Ensure array is float and replace inf/nan with nan for stat calculation\n",
        "    arr_batch = arr_batch.astype(np.float32)\n",
        "    arr_clean = np.where(np.isfinite(arr_batch), arr_batch, np.nan)\n",
        "\n",
        "    # Compute stats along the spatial dimensions (H, W) for each tile (N)\n",
        "    # Use nan functions to handle any potential NaNs introduced or still present\n",
        "    mean_vals = np.nanmean(arr_clean, axis=(1, 2))\n",
        "    median_vals = np.nanmedian(arr_clean, axis=(1, 2))\n",
        "    std_vals = np.nanstd(arr_clean, axis=(1, 2))\n",
        "    min_vals = np.nanmin(arr_clean, axis=(1, 2))\n",
        "    max_vals = np.nanmax(arr_clean, axis=(1, 2))\n",
        "\n",
        "    # Replace any resulting NaNs/Infs in stats with 0\n",
        "    mean_vals = np.nan_to_num(mean_vals, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    median_vals = np.nan_to_num(median_vals, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    std_vals = np.nan_to_num(std_vals, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    min_vals = np.nan_to_num(min_vals, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    max_vals = np.nan_to_num(max_vals, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "\n",
        "    # Stack stats into a single array (N, 5)\n",
        "    return np.stack([mean_vals, median_vals, std_vals, min_vals, max_vals], axis=1)\n",
        "\n",
        "################\n",
        "\n",
        "def process_and_return_tiles(df, is_train=True, tile_size=TILE_SIZE):\n",
        "    \"\"\"\n",
        "    Processes raster tiles from a DataFrame, pads them if necessary,\n",
        "    and returns them as a stacked numpy array.\n",
        "    Optionally returns labels for training data.\n",
        "    Adds a tqdm progress bar for tile reading.\n",
        "    \"\"\"\n",
        "    all_tiles = []\n",
        "    all_labels = []\n",
        "    skipped_count = 0\n",
        "\n",
        "    # Add tqdm to the loop iterating through DataFrame rows\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Tiles\", unit=\"file\"):\n",
        "        tile_path = row.get('tifPath') # Use .get for safety\n",
        "\n",
        "        if is_train:\n",
        "            # Ensure 'Target' column exists for training data\n",
        "            if 'Target' not in row:\n",
        "                 skipped_count += 1\n",
        "                 continue\n",
        "            label_vector = [row['Target']] # Assuming Target is a single value\n",
        "\n",
        "        # Check if file exists and is valid\n",
        "        if not tile_path or not os.path.exists(tile_path):\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with rasterio.open(tile_path) as src:\n",
        "                # Read all bands as float32\n",
        "                if src.count < 12:\n",
        "                     skipped_count += 1\n",
        "                     continue\n",
        "\n",
        "                raster = src.read(out_dtype=np.float32) # (bands, height, width)\n",
        "\n",
        "            raster[raster < 0] = np.nan # Simple example: set negative values to NaN\n",
        "\n",
        "            # Pad or Crop if dimensions don't match tile_size\n",
        "            bands, height, width = raster.shape\n",
        "\n",
        "            if height != tile_size or width != tile_size:\n",
        "                 raster_processed = np.full((bands, tile_size, tile_size), np.nan, dtype=np.float32)\n",
        "                 copy_height = min(height, tile_size)\n",
        "                 copy_width = min(width, tile_size)\n",
        "                 raster_processed[:, :copy_height, :copy_width] = raster[:, :copy_height, :copy_width]\n",
        "            else:\n",
        "                raster_processed = raster # No padding needed\n",
        "\n",
        "            all_tiles.append(raster_processed)\n",
        "\n",
        "            if is_train:\n",
        "                all_labels.append(label_vector)\n",
        "\n",
        "        except rasterio.errors.RasterioIOError as e:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "    # Stack tiles and labels into numpy arrays\n",
        "    if not all_tiles:\n",
        "        print(\"Processed 0 valid tiles.\")\n",
        "        expected_bands = 12 # Default assumption\n",
        "        if skipped_count > 0:\n",
        "             print(f\"Skipped {skipped_count} tiles.\")\n",
        "        return np.empty((0, expected_bands, tile_size, tile_size), dtype=np.float32), np.array([]) if is_train else None\n",
        "\n",
        "\n",
        "    all_tiles_array = np.stack(all_tiles)  # shape: (N, C, H, W)\n",
        "    all_labels_array = np.array(all_labels) if is_train else None  # shape: (N, labels)\n",
        "\n",
        "    print(f\"Successfully processed {len(all_tiles)} tiles.\")\n",
        "    if skipped_count > 0:\n",
        "         print(f\"Skipped {skipped_count} tiles due to errors or missing data.\")\n",
        "    return all_tiles_array, all_labels_array\n",
        "\n",
        "\n",
        "# Define a function to extract texture features for a single tile\n",
        "# This function will be called in parallel by joblib\n",
        "def _extract_texture_features_for_one_tile(tile_index, all_data_map, texture_targets):\n",
        "    \"\"\"\n",
        "    Helper function to extract texture features for a single tile across specified target layers.\n",
        "    Designed to be called in parallel. Suppresses specific RuntimeWarnings locally.\n",
        "    \"\"\"\n",
        "    # --- Set NumPy error handling to suppress warnings for invalid operations ---\n",
        "    # This is necessary because joblib uses separate processes, each needing its own setting.\n",
        "    # 'invalid' includes warnings like comparison with NaN, sqrt of negative, etc.\n",
        "    # 'divide' includes warnings like division by zero.\n",
        "    old_np_settings = np.seterr(invalid='ignore', divide='ignore')\n",
        "\n",
        "    try:\n",
        "        tile_texture_features = []\n",
        "        glcm_props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
        "        gabor_stats = ['mean', 'std', 'max', 'min']\n",
        "        lbp_bins = 8 + 2 # P=8 uniform\n",
        "        morph_props = ['OpeningRatio', 'ClosingRatio', 'ErosionRatio', 'DilationRatio']\n",
        "        entropy_stats = ['Shannon', 'LocalMean', 'LocalStd']\n",
        "        edge_types = ['canny', 'sobel', 'prewitt', 'roberts']\n",
        "        edge_stats = ['sum', 'mean', 'std']\n",
        "        # Recalculate expected count for safety in padding/checking\n",
        "        expected_features_count_per_tile = 0\n",
        "        for target_name in texture_targets:\n",
        "           glcm_feats_count = len([1])*len([0, 45, 90, 135])*len(glcm_props)\n",
        "           gabor_feats_count = len([0.1, 0.3, 0.5])*len(gabor_stats)\n",
        "           lbp_feats_count = lbp_bins\n",
        "           morph_feats_count = len(morph_props)\n",
        "           entropy_feats_count = len(entropy_stats)\n",
        "           edge_feats_count = len(edge_types)*len(edge_stats)\n",
        "           expected_features_count_per_tile += (glcm_feats_count + gabor_feats_count + lbp_feats_count + morph_feats_count + entropy_feats_count + edge_feats_count)\n",
        "\n",
        "\n",
        "        for target_name in texture_targets:\n",
        "            if target_name in all_data_map:\n",
        "                target_data_2d = all_data_map[target_name][tile_index, :, :]\n",
        "            else:\n",
        "                # If a texture target isn't found (unexpected), append zero placeholders\n",
        "                glcm_feats_count = len([1])*len([0, 45, 90, 135])*len(glcm_props)\n",
        "                gabor_feats_count = len([0.1, 0.3, 0.5])*len(gabor_stats)\n",
        "                lbp_feats_count = lbp_bins\n",
        "                morph_feats_count = len(morph_props)\n",
        "                entropy_feats_count = len(entropy_stats)\n",
        "                edge_feats_count = len(edge_types)*len(edge_stats)\n",
        "                tile_texture_features.extend([0.0] * (glcm_feats_count + gabor_feats_count + lbp_feats_count + morph_feats_count + entropy_feats_count + edge_feats_count))\n",
        "                continue # Move to the next texture target\n",
        "\n",
        "\n",
        "            # Apply all texture functions to this 2D data slice\n",
        "            # Each function is responsible for handling potential errors/size issues on its own\n",
        "            tile_texture_features.extend(extract_glcm_features(target_data_2d, distances=[1], angles=[0, 45, 90, 135]))\n",
        "            tile_texture_features.extend(extract_gabor_features(target_data_2d, frequencies=[0.1, 0.3, 0.5]))\n",
        "            tile_texture_features.extend(extract_local_binary_pattern(target_data_2d, P=8, R=1))\n",
        "            tile_texture_features.extend(extract_morphological_features(target_data_2d))\n",
        "            tile_texture_features.extend(extract_entropy_features(target_data_2d))\n",
        "            tile_texture_features.extend(extract_edge_features(target_data_2d))\n",
        "\n",
        "        # Final check within the worker function: ensure the number of features matches expected\n",
        "        # This is a fallback for internal logic errors in texture functions not returning correct size consistently\n",
        "        if len(tile_texture_features) != expected_features_count_per_tile:\n",
        "             # Pad/trim if mismatch\n",
        "             tile_texture_features = tile_texture_features[:expected_features_count_per_tile] + [0.0] * (expected_features_count_per_tile - len(tile_texture_features))\n",
        "\n",
        "\n",
        "        return tile_texture_features # Return the feature list for this tile\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected error during texture extraction for THIS tile\n",
        "        # Return placeholder features if a severe error occurred for this tile\n",
        "        # print(f\"Severe error extracting texture features for tile index {tile_index}: {e}\") # Avoid excessive printing\n",
        "        # Recalculate expected count here for robustness in case of early failure\n",
        "        glcm_props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
        "        gabor_stats = ['mean', 'std', 'max', 'min']\n",
        "        lbp_bins = 8 + 2 # P=8 uniform\n",
        "        morph_props = ['OpeningRatio', 'ClosingRatio', 'ErosionRatio', 'DilationRatio']\n",
        "        entropy_stats = ['Shannon', 'LocalMean', 'LocalStd']\n",
        "        edge_types = ['canny', 'sobel', 'prewitt', 'roberts']\n",
        "        edge_stats = ['sum', 'mean', 'std']\n",
        "        expected_features_count_per_tile = 0\n",
        "        for target_name in texture_targets:\n",
        "           glcm_feats_count = len([1])*len([0, 45, 90, 135])*len(glcm_props)\n",
        "           gabor_feats_count = len([0.1, 0.3, 0.5])*len(gabor_stats)\n",
        "           lbp_feats_count = lbp_bins\n",
        "           morph_feats_count = len(morph_props)\n",
        "           entropy_feats_count = len(entropy_stats)\n",
        "           edge_feats_count = len(edge_types)*len(edge_stats)\n",
        "           expected_features_count_per_tile += (glcm_feats_count + gabor_feats_count + lbp_feats_count + morph_feats_count + entropy_feats_count + edge_feats_count)\n",
        "\n",
        "        return [0.0] * expected_features_count_per_tile # Return zeros if any error occurs for this tile\n",
        "\n",
        "\n",
        "    finally:\n",
        "        # --- Restore NumPy error handling settings for this process ---\n",
        "        np.seterr(**old_np_settings)\n",
        "\n",
        "\n",
        "def extract_features_from_tiles(tiles):\n",
        "    \"\"\"\n",
        "    Extract statistical features and texture features using parallel processing.\n",
        "    Texture extraction runs in parallel processes with warning suppression.\n",
        "    Suppresses RuntimeWarnings during vectorized index calculations.\n",
        "    \"\"\"\n",
        "    batch_size = 50\n",
        "    num_tiles = tiles.shape[0]\n",
        "    if num_tiles == 0:\n",
        "        print(\"No tiles provided for feature extraction. Returning empty DataFrame.\")\n",
        "        # Build column list to return an empty DataFrame with correct columns\n",
        "        # This needs to match the order used when tiles *are* present\n",
        "        stat_names = ['mean', 'median', 'std', 'min', 'max']\n",
        "\n",
        "        # Define band names (consistent order)\n",
        "        band_names_in_order = [\n",
        "             'B1_Coastal', 'B2_Blue', 'B3_Green', 'B4_Red', 'B5_RedEdge1', 'B6_RedEdge2',\n",
        "             'B7_RedEdge3', 'B8_NIR', 'B8A_NarrowNIR', 'B9_WaterVapour', 'B11_SWIR1', 'B12_SWIR2'\n",
        "        ]\n",
        "        # Define index names in the order they are computed by add_index\n",
        "        index_names_template = [\n",
        "             'NDVI', 'SAVI', 'EVI', 'S2REP', 'CCCI', 'NDWI', # Original\n",
        "             'NDRE', 'MMSR', 'GNDVI', 'EVI2', 'NGRDI', 'MNDWI', 'OSAVI', 'WDRVI', # Vegetation\n",
        "             'TGI', 'GCVI', 'RGVI', 'ARVI', 'SIPI', 'RENDVI', 'MRESR', 'RYI',\n",
        "             'NDYI', 'DYI', 'ACI', 'CVI', 'AVI', 'SI', 'BSI', 'MTCI',\n",
        "             'NPCRI', 'MCARI', 'TCARI', 'PVI', 'BAI', 'MTVI2', 'NDSI', 'MRENDVI',\n",
        "             'NDVI_RE', 'CI_RE', 'NDMI', 'TNDVI', 'VDVI', 'NBR', 'TVI', 'EXG',\n",
        "             'PSRI', 'RDVI',\n",
        "             'RATIO_B1_B3', 'RATIO_B1_B5', 'RATIO_B11_B12', # Ratios\n",
        "             'RATIO_B5_B4', 'RATIO_B7_B5', 'RATIO_B7_B6', 'RATIO_B8_B4',\n",
        "             'LAI', 'CAB', 'CAR', 'CRI1', 'CRI2', 'PRI', 'LWI', 'MSI', 'WI', # Biophysical/Other\n",
        "             'FE2', 'FE3', 'CLAY', # Geological\n",
        "             'ATSAVI', 'GEMI', 'IPVI', 'DVI', 'RVI', 'SR', 'TSAVI', 'MSAVI', 'MSAVI2', # Advanced Veg\n",
        "             'GARI', 'GLI', 'NLI',\n",
        "             'AWEI_NS', 'AWEI_SH', 'WI2015', 'TCW', 'TCB', 'TCG', # Water\n",
        "             'NDTI', 'STI', 'CI_Coloration', 'HUE', 'SAT', 'INT', # Geological/Soil\n",
        "             'NDBI', 'EBBI', 'UI', 'IBI', # Urban\n",
        "             'S3', 'NDSII', 'SNOWMAP', # Snow\n",
        "             'VARI', 'GVMI' # Atmospheric\n",
        "        ]\n",
        "\n",
        "        # Combine band and index names for stats columns\n",
        "        all_data_names_template = band_names_in_order + index_names_template\n",
        "        index_stat_columns = [f\"{name}_{stat}\" for name in all_data_names_template for stat in stat_names]\n",
        "\n",
        "        # Texture feature column names (must match the order and types extracted in _extract_texture_features_for_one_tile)\n",
        "        texture_columns = []\n",
        "        texture_targets = ['B4_Red', 'B8_NIR', 'B11_SWIR1', 'NDVI'] # Must match names used in helper function\n",
        "        glcm_props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
        "        gabor_stats = ['mean', 'std', 'max', 'min']\n",
        "        lbp_bins = 8 + 2 # P=8 uniform\n",
        "        morph_props = ['OpeningRatio', 'ClosingRatio', 'ErosionRatio', 'DilationRatio']\n",
        "        entropy_stats = ['Shannon', 'LocalMean', 'LocalStd']\n",
        "        edge_types = ['canny', 'sobel', 'prewitt', 'roberts']\n",
        "        edge_stats = ['sum', 'mean', 'std']\n",
        "\n",
        "        for target_name in texture_targets:\n",
        "            for d in [1]: # GLCM distances\n",
        "                for a in [0, 45, 90, 135]: # GLCM angles\n",
        "                    texture_columns.extend([f\"{target_name}_GLCM_d{d}a{a}_{prop}\" for prop in glcm_props])\n",
        "            for freq in [0.1, 0.3, 0.5]: # Gabor frequencies\n",
        "                texture_columns.extend([f\"{target_name}_Gabor_Freq{freq}_{stat}\" for stat in gabor_stats])\n",
        "            texture_columns.extend([f\"{target_name}_LBP_Hist_bin{i}\" for i in range(lbp_bins)])\n",
        "            texture_columns.extend([f\"{target_name}_Morph_{prop}\" for prop in morph_props])\n",
        "            texture_columns.extend([f\"{target_name}_Entropy_{stat}\" for stat in entropy_stats])\n",
        "            for edge_type in edge_types:\n",
        "                 texture_columns.extend([f\"{target_name}_Edge_{edge_type}_{stat}\" for stat in edge_stats])\n",
        "\n",
        "\n",
        "        all_columns = index_stat_columns + texture_columns\n",
        "        return pd.DataFrame([], columns=all_columns)\n",
        "\n",
        "\n",
        "    print(f\"Starting feature extraction for {num_tiles} tiles...\")\n",
        "\n",
        "    # --- Set NumPy error handling to suppress warnings for vectorized index calculations ---\n",
        "    # This applies to the main process only. Workers handle their own settings.\n",
        "    # Suppress 'invalid' (e.g., comparison with NaN, sqrt of negative) and 'divide' (e.g., division by zero).\n",
        "    old_np_settings_main = np.seterr(invalid='ignore', divide='ignore')\n",
        "\n",
        "    try:\n",
        "        # --- Extract Bands (Batch) ---\n",
        "        if tiles.shape[1] < 12:\n",
        "            print(f\"⚠️ Error: Tiles array has {tiles.shape[1]} bands, but at least 12 are required for feature extraction.\")\n",
        "            return pd.DataFrame([]) # Return empty DF on error\n",
        "\n",
        "        # Extract bands by index - These operations are vectorized and usually fast\n",
        "        b1 = tiles[:, 0, :, :]     # B1 (Coastal aerosol)\n",
        "        b2 = tiles[:, 1, :, :]   # B2 (Blue)\n",
        "        b3 = tiles[:, 2, :, :]  # B3 (Green)\n",
        "        b4 = tiles[:, 3, :, :]    # B4 (Red)\n",
        "        b5 = tiles[:, 4, :, :]     # B5 (Red Edge 1)\n",
        "        b6 = tiles[:, 5, :, :]     # B6 (Red Edge 2)\n",
        "        b7 = tiles[:, 6, :, :]     # B7 (Red Edge 3)\n",
        "        b8 = tiles[:, 7, :, :]    # B8 (NIR)\n",
        "        b8a = tiles[:, 8, :, :]    # B8A (Narrow NIR)\n",
        "        b9 = tiles[:, 9, :, :]     # B9 (Water vapour)\n",
        "        b11 = tiles[:, 10, :, :]   # B11 (SWIR 1)\n",
        "        b12 = tiles[:, 11, :, :]   # B12 (SWIR 2)\n",
        "\n",
        "\n",
        "        # --- Compute Indices (Batch) ---\n",
        "        indices_batch = []\n",
        "        index_names_in_order = []\n",
        "\n",
        "        def add_index(func, name, *args):\n",
        "            \"\"\"Helper to compute index and add to list and names list, handling errors.\"\"\"\n",
        "            try:\n",
        "                idx_result = func(*args)\n",
        "                # Ensure result is float32 and handle potential NaNs/Infs from computation\n",
        "                idx_result = np.asarray(idx_result, dtype=np.float32)\n",
        "                indices_batch.append(idx_result)\n",
        "                index_names_in_order.append(name)\n",
        "            except Exception as e:\n",
        "                # print(f\"⚠️ Error computing index '{name}': {e}. Appending NaN array.\") # Print less verbosely\n",
        "                # Append an array of NaNs or a placeholder of the correct shape\n",
        "                if tiles.shape[0] > 0:\n",
        "                     placeholder = np.full_like(tiles[:, 0, :, :], np.nan, dtype=np.float32)\n",
        "                else:\n",
        "                     placeholder = np.empty((0, TILE_SIZE, TILE_SIZE), dtype=np.float32) # Fallback\n",
        "\n",
        "                indices_batch.append(placeholder)\n",
        "                index_names_in_order.append(name)\n",
        "\n",
        "\n",
        "        print(\"Computing vegetation and other indices...\")\n",
        "        # These calculations are vectorized across tiles (N, H, W)\n",
        "        # Warnings for invalid/divide operations will be suppressed by np.seterr above\n",
        "        add_index(extract_ndvi, 'NDVI', b8, b4)\n",
        "        add_index(extract_savi, 'SAVI', b8, b4)\n",
        "        add_index(extract_evi, 'EVI', b8, b4, b2)\n",
        "        add_index(extract_s2rep, 'S2REP', b4, b5, b6, b7)\n",
        "        add_index(extract_ccci, 'CCCI', b4, b5, b8)\n",
        "        add_index(extract_ndwi, 'NDWI', b3, b8)\n",
        "\n",
        "        add_index(extract_ndre, 'NDRE', b8, b6)\n",
        "        add_index(extract_mmsr, 'MMSR', b8, b4)\n",
        "        add_index(extract_gndvi, 'GNDVI', b8, b3)\n",
        "        add_index(extract_evi2, 'EVI2', b8, b4)\n",
        "        add_index(extract_ngrdi, 'NGRDI', b3, b4)\n",
        "        add_index(extract_mndwi, 'MNDWI', b3, b11)\n",
        "        add_index(extract_osavi, 'OSAVI', b8, b4)\n",
        "        add_index(extract_wdrvi, 'WDRVI', b8, b4)\n",
        "        add_index(extract_tgi, 'TGI', b3, b4, b2)\n",
        "        add_index(extract_gcvi, 'GCVI', b8, b3)\n",
        "        add_index(extract_rgvi, 'RGVI', b1, b3, b4, b5, b7)\n",
        "        add_index(extract_arvi, 'ARVI', b8, b4, b2)\n",
        "        add_index(extract_sipi, 'SIPI', b8, b2, b4)\n",
        "        add_index(extract_rendvi, 'RENDVI', b6, b5)\n",
        "        add_index(extract_mresr, 'MRESR', b6, b1, b5)\n",
        "        add_index(extract_ryi, 'RYI', b3, b2)\n",
        "        add_index(extract_ndyi, 'NDYI', b3, b2)\n",
        "        add_index(extract_dyi, 'DYI', b3, b2)\n",
        "        add_index(extract_aci, 'ACI', b8, b4, b3)\n",
        "        add_index(extract_cvi, 'CVI', b8, b3, b4)\n",
        "        add_index(extract_avi, 'AVI', b8, b4)\n",
        "        add_index(extract_si, 'SI', b2, b3, b4)\n",
        "        add_index(extract_bsi, 'BSI', b11, b4, b8, b2)\n",
        "        add_index(extract_mtci, 'MTCI', b6, b5, b4)\n",
        "        add_index(extract_npcri, 'NPCRI', b4, b2)\n",
        "        add_index(extract_mcari, 'MCARI', b5, b4, b3)\n",
        "        add_index(extract_tcari, 'TCARI', b5, b4, b3)\n",
        "        add_index(extract_pvi, 'PVI', b8, b4)\n",
        "        add_index(extract_bai, 'BAI', b4, b8)\n",
        "        add_index(extract_mtvi2, 'MTVI2', b8, b3, b4)\n",
        "        add_index(extract_ndsi, 'NDSI', b3, b11)\n",
        "        add_index(extract_mrendvi, 'MRENDVI', b6, b5, b1)\n",
        "        add_index(extract_ndvi_re, 'NDVI_RE', b8, b5)\n",
        "        add_index(extract_ci_re, 'CI_RE', b8, b5)\n",
        "        add_index(extract_ndmi, 'NDMI', b8, b11)\n",
        "        add_index(extract_tndvi, 'TNDVI', b8, b4)\n",
        "        add_index(extract_vdvi, 'VDVI', b3, b4, b2)\n",
        "        add_index(extract_nbr, 'NBR', b8, b11)\n",
        "        add_index(extract_tvi, 'TVI', b6, b3, b4)\n",
        "        add_index(extract_exg, 'EXG', b3, b4, b2)\n",
        "        add_index(extract_psri, 'PSRI', b4, b2, b6)\n",
        "        add_index(extract_rdvi, 'RDVI', b8, b4)\n",
        "        add_index(extract_ratio_b1_b3, 'RATIO_B1_B3', b1, b3)\n",
        "        add_index(extract_ratio_b1_b5, 'RATIO_B1_B5', b1, b5)\n",
        "        add_index(extract_ratio_b11_b12, 'RATIO_B11_B12', b11, b12)\n",
        "        add_index(extract_ratio_b5_b4, 'RATIO_B5_B4', b5, b4)\n",
        "        add_index(extract_ratio_b7_b5, 'RATIO_B7_B5', b7, b5)\n",
        "        add_index(extract_ratio_b7_b6, 'RATIO_B7_B6', b7, b6)\n",
        "        add_index(extract_ratio_b8_b4, 'RATIO_B8_B4', b8, b4)\n",
        "        add_index(extract_lai, 'LAI', b8, b4)\n",
        "        add_index(extract_cab, 'CAB', b5, b4)\n",
        "        add_index(extract_car, 'CAR', b2, b3, b4)\n",
        "        add_index(extract_cri1, 'CRI1', b2, b5)\n",
        "        add_index(extract_cri2, 'CRI2', b2, b7)\n",
        "        add_index(extract_pri, 'PRI', b2, b3)\n",
        "        add_index(extract_lwi, 'LWI', b8, b11)\n",
        "        add_index(extract_msi, 'MSI', b11, b8)\n",
        "        add_index(extract_wi, 'WI', b8, b3)\n",
        "        add_index(extract_fe2, 'FE2', b4, b8)\n",
        "        add_index(extract_fe3, 'FE3', b2, b4)\n",
        "        add_index(extract_clay, 'CLAY', b11, b12)\n",
        "        add_index(extract_atsavi, 'ATSAVI', b8, b4)\n",
        "        add_index(extract_gemi, 'GEMI', b8, b4)\n",
        "        add_index(extract_ipvi, 'IPVI', b8, b4)\n",
        "        add_index(extract_dvi, 'DVI', b8, b4)\n",
        "        add_index(extract_rvi, 'RVI', b8, b4)\n",
        "        add_index(extract_sr, 'SR', b8, b4)\n",
        "        add_index(extract_tsavi, 'TSAVI', b8, b4)\n",
        "        add_index(extract_msavi, 'MSAVI', b8, b4)\n",
        "        add_index(extract_msavi2, 'MSAVI2', b8, b4)\n",
        "        add_index(extract_gari, 'GARI', b8, b3, b4, b2)\n",
        "        add_index(extract_gli, 'GLI', b3, b4, b2)\n",
        "        add_index(extract_nli, 'NLI', b8, b4)\n",
        "        add_index(extract_awei_ns, 'AWEI_NS', b3, b11, b8, b12)\n",
        "        add_index(extract_awei_sh, 'AWEI_SH', b2, b3, b11, b8, b12)\n",
        "        add_index(extract_wi2015, 'WI2015', b3, b8, b11, b12)\n",
        "        add_index(extract_tcw, 'TCW', b2, b3, b4, b8, b11, b12)\n",
        "        add_index(extract_tcb, 'TCB', b2, b3, b4, b8, b11, b12)\n",
        "        add_index(extract_tcg, 'TCG', b2, b3, b4, b8, b11, b12)\n",
        "        add_index(extract_ndti, 'NDTI', b11, b12)\n",
        "        add_index(extract_sti, 'STI', b11, b12)\n",
        "        add_index(extract_ci, 'CI_Coloration', b4, b3)\n",
        "        add_index(extract_hue, 'HUE', b2, b3, b4)\n",
        "        add_index(extract_sat, 'SAT', b2, b3, b4)\n",
        "        add_index(extract_int, 'INT', b2, b3, b4)\n",
        "        add_index(extract_ndbi, 'NDBI', b11, b8)\n",
        "        add_index(extract_ebbi, 'EBBI', b11, b8, b4)\n",
        "        add_index(extract_ui, 'UI', b11, b8)\n",
        "        add_index(extract_ibi, 'IBI', b11, b8, b3, b4)\n",
        "        add_index(extract_s3, 'S3', b3, b4, b8)\n",
        "        add_index(extract_ndsii, 'NDSII', b3, b4)\n",
        "        add_index(extract_snowmap, 'SNOWMAP', b2, b11)\n",
        "        add_index(extract_vari, 'VARI', b3, b4, b2)\n",
        "        add_index(extract_gvmi, 'GVMI', b8, b11)\n",
        "\n",
        "\n",
        "        # Prepend original bands to the list for batch stats\n",
        "        bands_batch_data = [b1, b2, b3, b4, b5, b6, b7, b8, b8a, b9, b11, b12]\n",
        "        band_names_in_order = [\n",
        "            'B1_Coastal', 'B2_Blue', 'B3_Green', 'B4_Red', 'B5_RedEdge1', 'B6_RedEdge2',\n",
        "            'B7_RedEdge3', 'B8_NIR', 'B8A_NarrowNIR', 'B9_WaterVapour', 'B11_SWIR1', 'B12_SWIR2'\n",
        "        ]\n",
        "\n",
        "        all_data_batch = bands_batch_data + indices_batch\n",
        "        all_data_names = band_names_in_order + index_names_in_order # Combined list of names\n",
        "\n",
        "        # Create a dictionary mapping the combined names to the batch arrays for texture lookup\n",
        "        all_data_map = dict(zip(all_data_names, all_data_batch))\n",
        "\n",
        "\n",
        "        # Compute stats for all bands and indices in a batch\n",
        "        print(\"Calculating band and index statistics...\")\n",
        "        # This part is still vectorized NumPy and should be fast enough.\n",
        "        all_stats_batch = [batch_stats(data) for data in all_data_batch]\n",
        "\n",
        "        # Concatenate stats into a single array (N, total_bands_indices * 5)\n",
        "        index_features_array = np.concatenate(all_stats_batch, axis=1)\n",
        "\n",
        "\n",
        "        # --- Extract Texture Features (Per Tile) using Joblib ---\n",
        "        texture_targets = ['B4_Red', 'B8_NIR', 'B11_SWIR1', 'NDVI'] # Layers for texture\n",
        "\n",
        "        print(f\"Extracting texture features per tile using {os.cpu_count()} cores...\")\n",
        "\n",
        "        # Use joblib.Parallel for the per-tile texture extraction\n",
        "        # The _extract_texture_features_for_one_tile function handles its own errors and warning suppression\n",
        "        texture_features_list = Parallel(n_jobs=-1, verbose=11)(\n",
        "             delayed(_extract_texture_features_for_one_tile)(i, all_data_map, texture_targets)\n",
        "             for i in range(num_tiles)\n",
        "        )\n",
        "\n",
        "        # Stack texture features and handle potential errors/padding\n",
        "        if not texture_features_list:\n",
        "             texture_features_array = np.empty((num_tiles, 0), dtype=np.float32)\n",
        "        else:\n",
        "             # Ensure consistent length before stacking\n",
        "             # Recalculate expected texture feature count for one tile for robustness\n",
        "             glcm_props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
        "             gabor_stats = ['mean', 'std', 'max', 'min']\n",
        "             lbp_bins = 8 + 2\n",
        "             morph_props = ['OpeningRatio', 'ClosingRatio', 'ErosionRatio', 'DilationRatio']\n",
        "             entropy_stats = ['Shannon', 'LocalMean', 'LocalStd']\n",
        "             edge_types = ['canny', 'sobel', 'prewitt', 'roberts']\n",
        "             edge_stats = ['sum', 'mean', 'std']\n",
        "             expected_texture_features_count_per_tile = 0\n",
        "             for target_name in texture_targets:\n",
        "                glcm_feats_count = len([1])*len([0, 45, 90, 135])*len(glcm_props)\n",
        "                gabor_feats_count = len([0.1, 0.3, 0.5])*len(gabor_stats)\n",
        "                lbp_feats_count = lbp_bins\n",
        "                morph_feats_count = len(morph_props)\n",
        "                entropy_feats_count = len(entropy_stats)\n",
        "                edge_feats_count = len(edge_types)*len(edge_stats)\n",
        "                expected_texture_features_count_per_tile += (glcm_feats_count + gabor_feats_count + lbp_feats_count + morph_feats_count + entropy_feats_count + edge_feats_count)\n",
        "\n",
        "             expected_len = expected_texture_features_count_per_tile # Use the pre-calculated length\n",
        "\n",
        "\n",
        "             # Pad any feature lists that are shorter than the expected length\n",
        "             # (Features that failed extraction in a worker process will be [0.0] * expected_len already due to _extract_texture_features_for_one_tile's error handling)\n",
        "             # This step primarily ensures consistency if the expected_len calculation differs between main and worker, or if a rare edge case occurred.\n",
        "             padded_features_list = []\n",
        "             for features in texture_features_list:\n",
        "                 if len(features) < expected_len:\n",
        "                     padded_features_list.append(features + [0.0] * (expected_len - len(features)))\n",
        "                 elif len(features) > expected_len:\n",
        "                      padded_features_list.append(features[:expected_len])\n",
        "                 else:\n",
        "                     padded_features_list.append(features)\n",
        "\n",
        "             texture_features_array = np.array(padded_features_list, dtype=np.float32)\n",
        "\n",
        "\n",
        "        # --- Combine All Features ---\n",
        "        all_features_array = np.concatenate([index_features_array, texture_features_array], axis=1)\n",
        "\n",
        "        # --- Create Column Names ---\n",
        "        stat_names = ['mean', 'median', 'std', 'min', 'max']\n",
        "        # List of all bands and indices *names* for the stats part\n",
        "        all_data_names_actual = band_names_in_order + index_names_in_order # Use the actual names list generated\n",
        "\n",
        "        index_stat_columns = [f\"{name}_{stat}\" for name in all_data_names_actual for stat in stat_names]\n",
        "\n",
        "        # List of texture feature names: Use the texture_targets list and feature function structures\n",
        "        texture_columns = []\n",
        "        texture_targets_actual = ['B4_Red', 'B8_NIR', 'B11_SWIR1', 'NDVI'] # Re-declare for clarity\n",
        "        glcm_props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
        "        gabor_stats = ['mean', 'std', 'max', 'min']\n",
        "        lbp_bins = 8 + 2\n",
        "        morph_props = ['OpeningRatio', 'ClosingRatio', 'ErosionRatio', 'DilationRatio']\n",
        "        entropy_stats = ['Shannon', 'LocalMean', 'LocalStd']\n",
        "        edge_types = ['canny', 'sobel', 'prewitt', 'roberts']\n",
        "        edge_stats = ['sum', 'mean', 'std']\n",
        "\n",
        "        for target_name in texture_targets_actual:\n",
        "            for d in [1]:\n",
        "                for a in [0, 45, 90, 135]:\n",
        "                    texture_columns.extend([f\"{target_name}_GLCM_d{d}a{a}_{prop}\" for prop in glcm_props])\n",
        "            for freq in [0.1, 0.3, 0.5]:\n",
        "                texture_columns.extend([f\"{target_name}_Gabor_Freq{freq}_{stat}\" for stat in gabor_stats])\n",
        "            texture_columns.extend([f\"{target_name}_LBP_Hist_bin{i}\" for i in range(lbp_bins)])\n",
        "            texture_columns.extend([f\"{target_name}_Morph_{prop}\" for prop in morph_props])\n",
        "            texture_columns.extend([f\"{target_name}_Entropy_{stat}\" for stat in entropy_stats])\n",
        "            for edge_type in edge_types:\n",
        "                 texture_columns.extend([f\"{target_name}_Edge_{edge_type}_{stat}\" for stat in edge_stats])\n",
        "\n",
        "\n",
        "        all_columns = index_stat_columns + texture_columns\n",
        "\n",
        "        # Final check on column count\n",
        "        if all_features_array.shape[1] != len(all_columns):\n",
        "             print(f\"⚠️ CRITICAL MISMATCH: Extracted features count ({all_features_array.shape[1]}) does not match column names count ({len(all_columns)}).\")\n",
        "             print(f\"  Expected Index Stats Columns Count: {len(all_data_names_actual) * len(stat_names)}\")\n",
        "             print(f\"  Actual Index Features Array Columns: {index_features_array.shape[1]}\")\n",
        "             print(f\"  Expected Texture Columns Count: {expected_texture_features_count_per_tile}\")\n",
        "             print(f\"  Actual Texture Features Array Columns: {texture_features_array.shape[1]}\")\n",
        "             print(f\"  Total Expected Columns: {len(all_columns)}\")\n",
        "             print(f\"  Total Actual Columns: {all_features_array.shape[1]}\")\n",
        "             # Fallback to generic names if mismatch\n",
        "             return pd.DataFrame(all_features_array, columns=[f'feature_{i}' for i in range(all_features_array.shape[1])])\n",
        "\n",
        "        print(f\"Extracted {all_features_array.shape[1]} features for {num_tiles} tiles.\")\n",
        "        return pd.DataFrame(all_features_array, columns=all_columns)\n",
        "\n",
        "    finally:\n",
        "        # --- Restore NumPy error handling settings for the main process ---\n",
        "        np.seterr(**old_np_settings_main)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T14:39:15.967371Z",
          "iopub.execute_input": "2025-05-30T14:39:15.967831Z",
          "iopub.status.idle": "2025-05-30T14:39:18.550493Z",
          "shell.execute_reply.started": "2025-05-30T14:39:15.967807Z",
          "shell.execute_reply": "2025-05-30T14:39:18.549599Z"
        },
        "id": "mq4ej_4FpW9w"
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the actual local path to the S2Images folder within the downloaded dataset\n",
        "# Based on the original path '/kaggle/input/ivorycoast-byte-sizedagric/Côte d’Ivoire Byte-Sized Agric/S2Images/S2Images/'\n",
        "# The structure inside the downloaded 'paulnkamau_ivorycoast_byte_sizedagric' directory should mirror this.\n",
        "# The base path for the S2Images folder is likely:\n",
        "local_image_folder_base = os.path.join(\n",
        "    paulnkamau_ivorycoast_byte_sizedagric_path,\n",
        "    'Côte d’Ivoire Byte-Sized Agric', # Adjust this folder name if it differs in the downloaded path\n",
        "    'S2Images',\n",
        "    'S2Images'\n",
        ")\n",
        "\n",
        "# The prefix we want to remove from the original tifPath values is:\n",
        "kaggle_image_path_prefix = '/kaggle/input/ivorycoast-byte-sizedagric/Côte d’Ivoire Byte-Sized Agric/S2Images/S2Images/'\n",
        "\n",
        "# Replace the Kaggle prefix with the local base path for the image folder\n",
        "# Add os.sep to the local path to ensure it has a trailing slash appropriate for the OS\n",
        "train[\"tifPath\"] = train[\"tifPath\"].str.replace(\n",
        "    kaggle_image_path_prefix,\n",
        "    local_image_folder_base + os.sep,\n",
        "    regex=False\n",
        ")\n",
        "\n",
        "test[\"tifPath\"] = test[\"tifPath\"].str.replace(\n",
        "    kaggle_image_path_prefix,\n",
        "    local_image_folder_base + os.sep,\n",
        "    regex=False\n",
        ")\n",
        "\n",
        "print(\"tifPath columns updated to local paths.\")\n",
        "# Optional: Print a sample tifPath to verify\n",
        "print(\"\\nSample train tifPath after correction:\")\n",
        "print(train[\"tifPath\"].iloc[0])\n",
        "print(\"\\nSample test tifPath after correction:\")\n",
        "print(test[\"tifPath\"].iloc[0])\n",
        "\n",
        "\n",
        "# --- Now continue with your existing code, ensuring you have the feature extraction functions defined ---\n",
        "\n",
        "# Assuming TILE_SIZE is defined globally, e.g., from MAX_TILE_SIZE calculation\n",
        "try:\n",
        "    TILE_SIZE\n",
        "except NameError:\n",
        "    print(\"TILE_SIZE not defined. Using default 326.\")\n",
        "    TILE_SIZE = 326 # Default if MAX_TILE_SIZE was not computed earlier\n",
        "\n",
        "def process_and_return_tiles_with_df(df, is_train=True, tile_size=TILE_SIZE):\n",
        "    # ... (your existing process_and_return_tiles_with_df function code) ...\n",
        "    \"\"\"\n",
        "    Processes raster tiles from a DataFrame, padding if necessary.\n",
        "    Returns the subset of the original DataFrame corresponding to processed tiles,\n",
        "    the stacked numpy array of tiles, and labels (if is_train).\n",
        "    Handles missing paths and basic raster errors.\n",
        "    Includes tqdm progress bar and garbage collection.\n",
        "    \"\"\"\n",
        "    all_tiles = []\n",
        "    all_labels = []\n",
        "    processed_rows = [] # Store the original DataFrame rows that were successfully processed\n",
        "    skipped_count = 0\n",
        "\n",
        "    # Use tqdm on the provided DataFrame rows\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Tiles\", unit=\"file\"):\n",
        "        tile_path = row.get('tifPath')\n",
        "\n",
        "        label_vector = None # Default for test data\n",
        "        if is_train:\n",
        "            # Ensure 'Target' column exists for training data\n",
        "            if 'Target' not in row:\n",
        "                 skipped_count += 1\n",
        "                 # print(f\"Skipping row {index}: 'Target' column not found.\") # Debugging\n",
        "                 continue\n",
        "            label_vector = row['Target'] # Store as single value (string: 'Cocoa', 'Palm', 'Rubber')\n",
        "\n",
        "\n",
        "        # Check if file exists and is valid\n",
        "        if not tile_path or not os.path.exists(str(tile_path)):\n",
        "            skipped_count += 1\n",
        "            # print(f\"Skipping row {index}: Missing or invalid file path: {tile_path}\") # Debugging\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with rasterio.open(str(tile_path)) as src:\n",
        "                # Ensure the tile has at least 12 bands as expected by feature extraction\n",
        "                if src.count < 12:\n",
        "                     skipped_count += 1\n",
        "                     # print(f\"Skipping row {index} ({tile_path}): Expected at least 12 bands, found {src.count}.\") # Debugging\n",
        "                     continue\n",
        "\n",
        "                # Read all bands as float32\n",
        "                raster = src.read(out_dtype=np.float32) # (bands, height, width)\n",
        "\n",
        "            # Handle potential NoData values, often represented as specific numbers like -9999\n",
        "            # A simple way is to replace common NoData values or just keep NaNs\n",
        "            raster[raster < 0] = np.nan # Set negative values to NaN\n",
        "\n",
        "\n",
        "            # Pad or Crop if dimensions don't match tile_size\n",
        "            bands, height, width = raster.shape\n",
        "\n",
        "            if height != tile_size or width != tile_size:\n",
        "                 raster_processed = np.full((bands, tile_size, tile_size), np.nan, dtype=np.float32) # Use NaN for padding\n",
        "                 copy_height = min(height, tile_size)\n",
        "                 copy_width = min(width, tile_size)\n",
        "                 raster_processed[:, :copy_height, :copy_width] = raster[:, :copy_height, :copy_width]\n",
        "            else:\n",
        "                raster_processed = raster # No padding needed\n",
        "\n",
        "            all_tiles.append(raster_processed)\n",
        "            processed_rows.append(row) # Append the original row for later DataFrame reconstruction\n",
        "\n",
        "            if is_train:\n",
        "                all_labels.append(label_vector) # Append single label\n",
        "\n",
        "        except rasterio.errors.RasterioIOError:\n",
        "            # print(f\"Rasterio error processing {tile_path}: {e}\") # Suppress printing\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "        except Exception:\n",
        "            # print(f\"Unexpected error processing {tile_path}: {e}\") # Suppress printing\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "    # Stack tiles and create processed DataFrame\n",
        "    if not all_tiles:\n",
        "        print(\"Processed 0 valid tiles.\")\n",
        "        # Return empty DataFrames with original columns, and empty tiles/labels arrays\n",
        "        # Need a way to get columns if no rows were processed. Let's return empty stuff.\n",
        "        empty_df = df.iloc[[]].copy()\n",
        "        empty_tiles = np.empty((0, 12, tile_size, tile_size), dtype=np.float32) # Assume 12 bands if 0 processed\n",
        "        empty_labels = np.array([]) if is_train else None\n",
        "        if skipped_count > 0:\n",
        "             print(f\"Skipped {skipped_count} tiles.\")\n",
        "        return empty_df, empty_tiles, empty_labels\n",
        "\n",
        "\n",
        "    processed_df = pd.DataFrame(processed_rows).reset_index(drop=True) # Create DF from collected rows\n",
        "    all_tiles_array = np.stack(all_tiles)  # shape: (N, C, H, W)\n",
        "\n",
        "    # Clean up memory from the list of individual tile arrays\n",
        "    del all_tiles\n",
        "    gc.collect()\n",
        "\n",
        "    if is_train:\n",
        "         all_labels_array = np.array(all_labels) # shape: (N,)\n",
        "         del all_labels\n",
        "         gc.collect()\n",
        "    else:\n",
        "         all_labels_array = None # Keep as None for test\n",
        "\n",
        "    print(f\"Successfully processed {len(processed_df)} tiles.\")\n",
        "    if skipped_count > 0:\n",
        "         print(f\"Skipped {skipped_count} tiles due to errors or missing data.\")\n",
        "\n",
        "    # Final consistency checks\n",
        "    if is_train and len(processed_df) != len(all_labels_array):\n",
        "        raise ValueError(f\"Mismatch between processed DataFrame rows ({len(processed_df)}) and labels array size ({len(all_labels_array)}) for train.\")\n",
        "    if len(processed_df) != all_tiles_array.shape[0]:\n",
        "        raise ValueError(f\"Mismatch between processed DataFrame rows ({len(processed_df)}) and tiles array size ({all_tiles_array.shape[0]}).\")\n",
        "\n",
        "\n",
        "    return processed_df, all_tiles_array, all_labels_array # Return the processed DF slice too\n",
        "\n",
        "# Assuming extract_features_from_tiles and its helpers are defined previously\n",
        "\n",
        "\n",
        "# --- Filter DataFrames by Month ---\n",
        "print(\"\\nFiltering train and test data by month...\")\n",
        "train_dec_jan_feb = train[train['month'].isin(['Dec', 'Jan', 'Feb'])].copy()\n",
        "test_jan_feb = test[test['month'].isin(['Jan', 'Feb'])].copy()\n",
        "\n",
        "print(f\"Train data for Dec, Jan, Feb: {len(train_dec_jan_feb)} rows\")\n",
        "print(f\"Test data for Jan, Feb: {len(test_jan_feb)} rows\")"
      ],
      "metadata": {
        "id": "9TpQ9_83sOmT",
        "outputId": "41c9ae97-116c-4914-be05-9df0e7229294",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tifPath columns updated to local paths.\n",
            "\n",
            "Sample train tifPath after correction:\n",
            "/root/.cache/kagglehub/datasets/paulnkamau/ivorycoast-byte-sizedagric/versions/1/Côte d’Ivoire Byte-Sized Agric/S2Images/S2Images/train/s2_Rubber_ID_h14T0B_2024_01.tif\n",
            "\n",
            "Sample test tifPath after correction:\n",
            "/root/.cache/kagglehub/datasets/paulnkamau/ivorycoast-byte-sizedagric/versions/1/Côte d’Ivoire Byte-Sized Agric/S2Images/S2Images/test/s2_Unknown_ID_731818_2024_01.tif\n",
            "\n",
            "Filtering train and test data by month...\n",
            "Train data for Dec, Jan, Feb: 2859 rows\n",
            "Test data for Jan, Feb: 564 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bars\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Import garbage collection\n",
        "import gc\n",
        "\n",
        "# Assuming TILE_SIZE is defined globally, e.g., from MAX_TILE_SIZE calculation\n",
        "try:\n",
        "    TILE_SIZE\n",
        "except NameError:\n",
        "    print(\"TILE_SIZE not defined. Using default 326.\")\n",
        "    TILE_SIZE = 326 # Default if MAX_TILE_SIZE was not computed earlier\n",
        "\n",
        "def process_and_return_tiles_with_df(df, is_train=True, tile_size=TILE_SIZE):\n",
        "    \"\"\"\n",
        "    Processes raster tiles from a DataFrame, padding if necessary.\n",
        "    Returns the subset of the original DataFrame corresponding to processed tiles,\n",
        "    the stacked numpy array of tiles, and labels (if is_train).\n",
        "    Handles missing paths and basic raster errors.\n",
        "    Includes tqdm progress bar and garbage collection.\n",
        "    \"\"\"\n",
        "    all_tiles = []\n",
        "    all_labels = []\n",
        "    processed_rows = [] # Store the original DataFrame rows that were successfully processed\n",
        "    skipped_count = 0\n",
        "\n",
        "    # Use tqdm on the provided DataFrame rows\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Tiles\", unit=\"file\"):\n",
        "        tile_path = row.get('tifPath')\n",
        "\n",
        "        label_vector = None # Default for test data\n",
        "        if is_train:\n",
        "            # Ensure 'Target' column exists for training data\n",
        "            if 'Target' not in row:\n",
        "                 skipped_count += 1\n",
        "                 # print(f\"Skipping row {index}: 'Target' column not found.\") # Debugging\n",
        "                 continue\n",
        "            label_vector = row['Target'] # Store as single value (string: 'Cocoa', 'Palm', 'Rubber')\n",
        "\n",
        "\n",
        "        # Check if file exists and is valid\n",
        "        if not tile_path or not os.path.exists(str(tile_path)):\n",
        "            skipped_count += 1\n",
        "            # print(f\"Skipping row {index}: Missing or invalid file path: {tile_path}\") # Debugging\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            with rasterio.open(str(tile_path)) as src:\n",
        "                # Ensure the tile has at least 12 bands as expected by feature extraction\n",
        "                if src.count < 12:\n",
        "                     skipped_count += 1\n",
        "                     # print(f\"Skipping row {index} ({tile_path}): Expected at least 12 bands, found {src.count}.\") # Debugging\n",
        "                     continue\n",
        "\n",
        "                # Read all bands as float32\n",
        "                raster = src.read(out_dtype=np.float32) # (bands, height, width)\n",
        "\n",
        "            # Handle potential NoData values, often represented as specific numbers like -9999\n",
        "            # A simple way is to replace common NoData values or just keep NaNs\n",
        "            raster[raster < 0] = np.nan # Set negative values to NaN\n",
        "\n",
        "\n",
        "            # Pad or Crop if dimensions don't match tile_size\n",
        "            bands, height, width = raster.shape\n",
        "\n",
        "            if height != tile_size or width != tile_size:\n",
        "                 raster_processed = np.full((bands, tile_size, tile_size), np.nan, dtype=np.float32) # Use NaN for padding\n",
        "                 copy_height = min(height, tile_size)\n",
        "                 copy_width = min(width, tile_size)\n",
        "                 raster_processed[:, :copy_height, :copy_width] = raster[:, :copy_height, :copy_width]\n",
        "            else:\n",
        "                raster_processed = raster # No padding needed\n",
        "\n",
        "            all_tiles.append(raster_processed)\n",
        "            processed_rows.append(row) # Append the original row for later DataFrame reconstruction\n",
        "\n",
        "            if is_train:\n",
        "                all_labels.append(label_vector) # Append single label\n",
        "\n",
        "        except rasterio.errors.RasterioIOError:\n",
        "            # print(f\"Rasterio error processing {tile_path}: {e}\") # Suppress printing\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "        except Exception:\n",
        "            # print(f\"Unexpected error processing {tile_path}: {e}\") # Suppress printing\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "    # Stack tiles and create processed DataFrame\n",
        "    if not all_tiles:\n",
        "        print(\"Processed 0 valid tiles.\")\n",
        "        # Return empty DataFrames with original columns, and empty tiles/labels arrays\n",
        "        # Need a way to get columns if no rows were processed. Let's return empty stuff.\n",
        "        empty_df = df.iloc[[]].copy()\n",
        "        empty_tiles = np.empty((0, 12, tile_size, tile_size), dtype=np.float32) # Assume 12 bands if 0 processed\n",
        "        empty_labels = np.array([]) if is_train else None\n",
        "        if skipped_count > 0:\n",
        "             print(f\"Skipped {skipped_count} tiles.\")\n",
        "        return empty_df, empty_tiles, empty_labels\n",
        "\n",
        "\n",
        "    processed_df = pd.DataFrame(processed_rows).reset_index(drop=True) # Create DF from collected rows\n",
        "    all_tiles_array = np.stack(all_tiles)  # shape: (N, C, H, W)\n",
        "\n",
        "    # Clean up memory from the list of individual tile arrays\n",
        "    del all_tiles\n",
        "    gc.collect()\n",
        "\n",
        "    if is_train:\n",
        "         all_labels_array = np.array(all_labels) # shape: (N,)\n",
        "         del all_labels\n",
        "         gc.collect()\n",
        "    else:\n",
        "         all_labels_array = None # Keep as None for test\n",
        "\n",
        "    print(f\"Successfully processed {len(processed_df)} tiles.\")\n",
        "    if skipped_count > 0:\n",
        "         print(f\"Skipped {skipped_count} tiles due to errors or missing data.\")\n",
        "\n",
        "    # Final consistency checks\n",
        "    if is_train and len(processed_df) != len(all_labels_array):\n",
        "        raise ValueError(f\"Mismatch between processed DataFrame rows ({len(processed_df)}) and labels array size ({len(all_labels_array)}) for train.\")\n",
        "    if len(processed_df) != all_tiles_array.shape[0]:\n",
        "        raise ValueError(f\"Mismatch between processed DataFrame rows ({len(processed_df)}) and tiles array size ({all_tiles_array.shape[0]}).\")\n",
        "\n",
        "\n",
        "    return processed_df, all_tiles_array, all_labels_array # Return the processed DF slice too\n",
        "\n",
        "\n",
        "# --- Filter DataFrames by Month ---\n",
        "print(\"Filtering train and test data by month...\")\n",
        "train_dec_jan_feb = train[train['month'].isin(['Dec', 'Jan', 'Feb'])].copy()\n",
        "test_jan_feb = test[test['month'].isin(['Jan', 'Feb'])].copy()\n",
        "\n",
        "print(f\"Train data for Dec, Jan, Feb: {len(train_dec_jan_feb)} rows\")\n",
        "print(f\"Test data for Jan, Feb: {len(test_jan_feb)} rows\")\n",
        "\n",
        "\n",
        "# --- Process Tiles and Extract Features ---\n",
        "# Call the modified processing function\n",
        "print(\"\\nProcessing train data and collecting tiles...\")\n",
        "train_processed_df, train_tiles_array, train_labels = process_and_return_tiles_with_df(train_dec_jan_feb, is_train=True)\n",
        "\n",
        "print(\"\\nProcessing test data and collecting tiles...\")\n",
        "test_processed_df, test_tiles_array, test_labels = process_and_return_tiles_with_df(test_jan_feb, is_train=False) # test_labels should be None\n",
        "\n",
        "\n",
        "print(f\"\\nShape of train_processed_df: {train_processed_df.shape}\")\n",
        "print(f\"Shape of train_tiles_array: {train_tiles_array.shape}\")\n",
        "print(f\"Shape of train_labels: {train_labels.shape if train_labels is not None else None}\")\n",
        "\n",
        "print(f\"\\nShape of test_processed_df: {test_processed_df.shape}\")\n",
        "print(f\"Shape of test_tiles_array: {test_tiles_array.shape}\")\n",
        "print(f\"Shape of test_labels: {test_labels}\") # Should be None\n",
        "\n",
        "# Extract Features from the collected tiles\n",
        "# This function should be defined previously in your notebook\n",
        "# batch_size for feature extraction is distinct from the batching in process_and_return_tiles if used there\n",
        "feature_extract_batch_size = 50 # Adjust based on memory\n",
        "batch_size = 50\n",
        "\n",
        "print(\"\\nExtracting features for train data...\")\n",
        "# Check if there are tiles to process\n",
        "if train_tiles_array.shape[0] > 0:\n",
        "    X_train_features = extract_features_from_tiles(train_tiles_array)\n",
        "else:\n",
        "    print(\"No train tiles to extract features from. Returning empty DataFrame.\")\n",
        "    # Need to get expected columns somehow - perhaps run extract_features_from_tiles on a dummy empty array?\n",
        "    # Or manually define the expected columns. Manual definition is safer for consistency.\n",
        "    # Manually defining column names here (must match extract_features_from_tiles output structure)\n",
        "    stat_names = ['mean', 'median', 'std', 'min', 'max']\n",
        "    band_names_template = [\n",
        "         'B1_Coastal', 'B2_Blue', 'B3_Green', 'B4_Red', 'B5_RedEdge1', 'B6_RedEdge2',\n",
        "         'B7_RedEdge3', 'B8_NIR', 'B8A_NarrowNIR', 'B9_WaterVapour', 'B11_SWIR1', 'B12_SWIR2'\n",
        "    ]\n",
        "    index_names_template = [\n",
        "         'NDVI', 'SAVI', 'EVI', 'S2REP', 'CCCI', 'NDWI',\n",
        "         'NDRE', 'MMSR', 'GNDVI', 'EVI2', 'NGRDI', 'MNDWI', 'OSAVI', 'WDRVI',\n",
        "         'TGI', 'GCVI', 'RGVI', 'ARVI', 'SIPI', 'RENDVI', 'MRESR', 'RYI',\n",
        "         'NDYI', 'DYI', 'ACI', 'CVI', 'AVI', 'SI', 'BSI', 'MTCI',\n",
        "         'NPCRI', 'MCARI', 'TCARI', 'PVI', 'BAI', 'MTVI2', 'NDSI', 'MRENDVI',\n",
        "         'NDVI_RE', 'CI_RE', 'NDMI', 'TNDVI', 'VDVI', 'NBR', 'TVI', 'EXG',\n",
        "         'PSRI', 'RDVI',\n",
        "         'RATIO_B1_B3', 'RATIO_B1_B5', 'RATIO_B11_B12',\n",
        "         'RATIO_B5_B4', 'RATIO_B7_B5', 'RATIO_B7_B6', 'RATIO_B8_B4',\n",
        "         'LAI', 'CAB', 'CAR', 'CRI1', 'CRI2', 'PRI', 'LWI', 'MSI', 'WI',\n",
        "         'FE2', 'FE3', 'CLAY',\n",
        "         'ATSAVI', 'GEMI', 'IPVI', 'DVI', 'RVI', 'SR', 'TSAVI', 'MSAVI', 'MSAVI2',\n",
        "         'GARI', 'GLI', 'NLI',\n",
        "         'AWEI_NS', 'AWEI_SH', 'WI2015', 'TCW', 'TCB', 'TCG',\n",
        "         'NDTI', 'STI', 'CI_Coloration', 'HUE', 'SAT', 'INT',\n",
        "         'NDBI', 'EBBI', 'UI', 'IBI',\n",
        "         'S3', 'NDSII', 'SNOWMAP',\n",
        "         'VARI', 'GVMI'\n",
        "    ]\n",
        "    all_data_names_template = band_names_template + index_names_template\n",
        "    index_stat_columns = [f\"{name}_{stat}\" for name in all_data_names_template for stat in stat_names]\n",
        "\n",
        "    texture_columns_template = [] # Recalculate texture columns template\n",
        "    texture_targets = ['B4_Red', 'B8_NIR', 'B11_SWIR1', 'NDVI']\n",
        "    glcm_props = ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation']\n",
        "    gabor_stats = ['mean', 'std', 'max', 'min']\n",
        "    lbp_bins = 8 + 2\n",
        "    morph_props = ['OpeningRatio', 'ClosingRatio', 'ErosionRatio', 'DilationRatio']\n",
        "    entropy_stats = ['Shannon', 'LocalMean', 'LocalStd']\n",
        "    edge_types = ['canny', 'sobel', 'prewitt', 'roberts']\n",
        "    edge_stats = ['sum', 'mean', 'std']\n",
        "    for target_name in texture_targets:\n",
        "        for d in [1]:\n",
        "            for a in [0, 45, 90, 135]:\n",
        "                texture_columns_template.extend([f\"{target_name}_GLCM_d{d}a{a}_{prop}\" for prop in glcm_props])\n",
        "        for freq in [0.1, 0.3, 0.5]:\n",
        "            texture_columns_template.extend([f\"{target_name}_Gabor_Freq{freq}_{stat}\" for stat in gabor_stats])\n",
        "        texture_columns_template.extend([f\"{target_name}_LBP_Hist_bin{i}\" for i in range(lbp_bins)])\n",
        "        texture_columns_template.extend([f\"{target_name}_Morph_{prop}\" for prop in morph_props])\n",
        "        texture_columns_template.extend([f\"{target_name}_Entropy_{stat}\" for stat in entropy_stats])\n",
        "        for edge_type in edge_types:\n",
        "             texture_columns_template.extend([f\"{target_name}_Edge_{edge_type}_{stat}\" for stat in edge_stats])\n",
        "    all_columns_template = index_stat_columns + texture_columns_template\n",
        "\n",
        "    X_train_features = pd.DataFrame([], columns=all_columns_template) # Empty DF with expected columns\n",
        "\n",
        "# Release train tiles memory immediately after feature extraction\n",
        "del train_tiles_array\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "print(\"\\nExtracting features for test data...\")\n",
        "# Check if there are tiles to process\n",
        "if test_tiles_array.shape[0] > 0:\n",
        "    X_test_features = extract_features_from_tiles(test_tiles_array)\n",
        "else:\n",
        "    print(\"No test tiles to extract features from. Returning empty DataFrame.\")\n",
        "    # Use the same column template as train\n",
        "    X_test_features = pd.DataFrame([], columns=all_columns_template) # Empty DF with expected columns\n",
        "\n",
        "\n",
        "# Release test tiles memory immediately after feature extraction\n",
        "del test_tiles_array\n",
        "gc.collect()\n",
        "\n",
        "print(f\"\\nShape of X_train_features: {X_train_features.shape}\")\n",
        "print(f\"Shape of X_test_features: {X_test_features.shape}\")\n",
        "\n",
        "\n",
        "# --- Handle NaNs/Infs in Features (Imputation) ---\n",
        "print(\"\\nHandling NaN/Inf in features using median imputation from train data...\")\n",
        "\n",
        "# Check if train features are empty - if so, imputation is not possible/needed\n",
        "if X_train_features.shape[0] == 0:\n",
        "    print(\"Train features are empty, skipping imputation.\")\n",
        "    # No imputation possible. X_test_features will also likely be empty or full of NaNs.\n",
        "    # K-NN and prediction will be skipped later.\n",
        "\n",
        "else:\n",
        "    # Check for column mismatch between train and test features (shouldn't happen if templates match)\n",
        "    if not X_train_features.columns.equals(X_test_features.columns):\n",
        "        print(\"⚠️ Warning: Column mismatch between train and test features. Attempting to align.\")\n",
        "        # Align columns - this might drop columns unique to one set or fill with NaNs\n",
        "        common_cols = X_train_features.columns.intersection(X_test_features.columns)\n",
        "        X_train_features = X_train_features[common_cols].copy()\n",
        "        X_test_features = X_test_features[common_cols].copy()\n",
        "        print(f\"Aligned to {len(common_cols)} common columns.\")\n",
        "\n",
        "\n",
        "    # Replace Inf with NaN first in both train and test features\n",
        "    X_train_features = X_train_features.replace([np.inf, -np.inf], np.nan)\n",
        "    X_test_features = X_test_features.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Calculate medians *only* on the training data to avoid data leakage\n",
        "    medians = X_train_features.median()\n",
        "\n",
        "    # Identify columns where median is NaN (all train values were NaN)\n",
        "    cols_with_all_nan_train = medians[medians.isna()].index.tolist()\n",
        "\n",
        "    # Impute NaNs in both train and test features using train medians\n",
        "    # fillna returns a new DataFrame, reassign\n",
        "    X_train_features = X_train_features.fillna(medians)\n",
        "    X_test_features = X_test_features.fillna(medians)\n",
        "\n",
        "    # For any columns where median was NaN (all training values were NaN/Inf), fill remaining NaNs with 0\n",
        "    if cols_with_all_nan_train:\n",
        "        print(f\"Imputing remaining NaNs (from all-NaN/Inf train columns) with 0 for {len(cols_with_all_nan_train)} columns.\")\n",
        "        X_train_features[cols_with_all_nan_train] = X_train_features[cols_with_all_nan_train].fillna(0)\n",
        "        X_test_features[cols_with_all_nan_train] = X_test_features[cols_with_all_nan_train].fillna(0)\n",
        "\n",
        "    # Final check that no NaNs/Infs remain\n",
        "    if X_train_features.isnull().any().any() or (X_train_features == np.inf).any().any() or (X_train_features == -np.inf).any().any():\n",
        "         print(\"CRITICAL ERROR: NaN/Inf found in X_train_features after cleaning.\")\n",
        "    if X_test_features.isnull().any().any() or (X_test_features == np.inf).any().any() or (X_test_features == -np.inf).any().any():\n",
        "         print(\"CRITICAL ERROR: NaN/Inf found in X_test_features after cleaning.\")\n",
        "\n",
        "print(f\"\\nShape of X_train_features after cleaning: {X_train_features.shape}\")\n",
        "print(f\"Shape of X_test_features after cleaning: {X_test_features.shape}\")\n",
        "\n",
        "\n",
        "# --- Filter Train Data based on Test Similarity (K-NN approach) ---\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "K_filter = 50 # Number of neighbors to find for filtering train data\n",
        "print(f\"\\nFinding {K_filter}-nearest neighbors in train for each test image to filter train data...\")\n",
        "\n",
        "# Check if there's enough data to perform K-NN\n",
        "if X_train_features.shape[0] == 0 or X_test_features.shape[0] == 0 or K_filter > X_train_features.shape[0]:\n",
        "     print(\"Cannot perform K-NN filtering: insufficient data or K_filter is too large.\")\n",
        "     # If filtering is not possible, the \"filtered train data\" is either empty or maybe the original?\n",
        "     # Based on the goal, an empty filtered set seems consistent if there's no test data to match.\n",
        "     filtered_train_data = train.iloc[[]].copy()\n",
        "     print(\"Filtered train data is empty.\")\n",
        "else:\n",
        "    # Initialize NearestNeighbors model for filtering\n",
        "    # Using 'euclidean' distance as the metric for similarity in feature space\n",
        "    try:\n",
        "        nn_filter = NearestNeighbors(n_neighbors=K_filter, algorithm='auto', metric='euclidean', n_jobs=-1)\n",
        "\n",
        "        # Fit on the training features\n",
        "        nn_filter.fit(X_train_features)\n",
        "\n",
        "        # Find neighbors for each test sample\n",
        "        # kneighbors returns distances and indices\n",
        "        distances_filter, indices_filter = nn_filter.kneighbors(X_test_features)\n",
        "\n",
        "        # indices_filter has shape (num_test_samples, K_filter)\n",
        "        # We need to collect all unique train indices from this result\n",
        "        unique_train_indices_for_filtering = np.unique(indices_filter.flatten())\n",
        "\n",
        "        print(f\"Found {len(unique_train_indices_for_filtering)} unique train images among {K_filter}-nearest neighbors for {X_test_features.shape[0]} test images.\")\n",
        "\n",
        "        # Select the corresponding rows from the *processed* train DataFrame\n",
        "        # The indices in unique_train_indices_for_filtering are the row indices in X_train_features,\n",
        "        # which correspond directly to the indices in train_processed_df because they were processed in order.\n",
        "        filtered_train_data = train_processed_df.iloc[unique_train_indices_for_filtering].copy()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during K-NN filtering: {e}\")\n",
        "        # Return empty filtered data in case of error\n",
        "        filtered_train_data = train.iloc[[]].copy()\n",
        "        print(\"Filtered train data is empty due to error.\")\n",
        "\n",
        "\n",
        "print(f\"Shape of filtered_train_data: {filtered_train_data.shape}\")\n",
        "\n",
        "# --- Save Filtered Train Data CSV ---\n",
        "output_filtered_train_csv = 'filtered_train_dec_jan_feb_knn50.csv'\n",
        "print(f\"\\nSaving filtered train data to '{output_filtered_train_csv}'...\")\n",
        "try:\n",
        "    filtered_train_data.to_csv(output_filtered_train_csv, index=False)\n",
        "    print(\"Filtered train data saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving filtered train data: {e}\")\n",
        "\n",
        "\n",
        "# --- Generate KNN Prediction Submission ---\n",
        "\n",
        "# Use K=1 neighbor for prediction\n",
        "K_predict = 1\n",
        "print(f\"\\nFinding {K_predict}-nearest neighbor in train for each test image to generate predictions...\")\n",
        "\n",
        "# Check if there's enough data to perform K-NN prediction\n",
        "if X_train_features.shape[0] == 0 or X_test_features.shape[0] == 0 or K_predict > X_train_features.shape[0]:\n",
        "    print(\"Cannot perform 1-NN prediction: insufficient data or K_predict is too large.\")\n",
        "    # Create a submission file with default predictions (e.g., majority class from original train if needed)\n",
        "    # Or just predict 'Unknown' for the relevant test IDs\n",
        "    submission_df = test_processed_df[['ID']].copy() # Use IDs from processed test data\n",
        "    # Determine a default prediction - let's use the most frequent class from the original full train data\n",
        "    if not train['Target'].empty:\n",
        "         default_pred = train['Target'].mode()[0]\n",
        "         print(f\"Predicting default class '{default_pred}' due to missing data.\")\n",
        "         submission_df['Target'] = default_pred\n",
        "    else:\n",
        "         print(\"Cannot determine default class, predicting 'Unknown'.\")\n",
        "         submission_df['Target'] = 'Unknown'\n",
        "\n",
        "    print(\"Generated submission with default predictions.\")\n",
        "\n",
        "else:\n",
        "    # Initialize NearestNeighbors model for 1-NN prediction\n",
        "    try:\n",
        "        nn_predict = NearestNeighbors(n_neighbors=K_predict, algorithm='auto', metric='euclidean', n_jobs=-1)\n",
        "\n",
        "        # Fit on the training features\n",
        "        nn_predict.fit(X_train_features)\n",
        "\n",
        "        # Find the single nearest neighbor for each test sample\n",
        "        distances_predict, indices_predict = nn_predict.kneighbors(X_test_features)\n",
        "\n",
        "        # indices_predict has shape (num_test_samples, 1) -> flatten to (num_test_samples,)\n",
        "        one_nn_train_indices = indices_predict.flatten()\n",
        "\n",
        "        # Get the predicted labels by looking up the Target of the 1-NN in the processed train DataFrame\n",
        "        # Use iloc on the processed train_df, as indices align with X_train_features rows\n",
        "        predicted_labels = train_processed_df.iloc[one_nn_train_indices]['Target'].values\n",
        "\n",
        "        # Create the submission DataFrame\n",
        "        # Use the IDs from the processed test DataFrame (which match the order of X_test_features)\n",
        "        submission_df = test_processed_df[['ID']].copy()\n",
        "        submission_df['Target'] = predicted_labels\n",
        "\n",
        "        print(f\"Generated {len(submission_df)} predictions based on 1-NN.\")\n",
        "        # print(submission_df.head()) # Optional: show head\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during 1-NN prediction: {e}\")\n",
        "        # Fallback to default prediction if an error occurs during KNN\n",
        "        submission_df = test_processed_df[['ID']].copy() # Use IDs from processed test data\n",
        "        if not train['Target'].empty:\n",
        "             default_pred = train['Target'].mode()[0]\n",
        "             print(f\"Predicting default class '{default_pred}' due to error.\")\n",
        "             submission_df['Target'] = default_pred\n",
        "        else:\n",
        "             print(\"Cannot determine default class, predicting 'Unknown'.\")\n",
        "             submission_df['Target'] = 'Unknown'\n",
        "\n",
        "\n",
        "# --- Save Submission File ---\n",
        "output_submission_csv = 'submission_knn_jan_feb_analysis.csv'\n",
        "print(f\"\\nSaving submission file to '{output_submission_csv}'...\")\n",
        "try:\n",
        "    submission_df.to_csv(output_submission_csv, index=False)\n",
        "    print(\"Submission file saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving submission file: {e}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-30T14:39:18.551297Z",
          "iopub.execute_input": "2025-05-30T14:39:18.551757Z",
          "execution_failed": "2025-05-30T14:41:33.201Z"
        },
        "id": "3pLpcx7CpW92",
        "outputId": "96ca4ecb-6db1-4431-e359-0ec180c812bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463,
          "referenced_widgets": [
            "d2a56b85ab21428c8c3d5c577d114596",
            "28fe449f460d4fd993335bcc61495af5",
            "93956e2e6dbd4e23a6cf597c4ae15357",
            "6d09186d7fbb45bcb1ed457e8aa9a956",
            "7a3d955d90d54692b0006ae3f0bd92fb",
            "3a1deb195c3a470ab7d1b78d01d84eac",
            "b0d28871ee3641dcade2b29faa05c56e",
            "d15703ddb81941338c25e608d7ab32f2",
            "76af2a8a0eb845989e71cd7b09f88cfc",
            "8831764a656b440496e58ef3a9943830",
            "efff88f76bf54953a2bd7e79418f8e55",
            "04ab451cc1a54ca4a7da384bb600d7a5",
            "dc47fbb8c8a544d18ccf179f39c90e2d",
            "b6f86fc0eafa49998436d0f5f552f5a0",
            "ffc1e47466214537a8ced98a8c03aaf0",
            "3c7672bef5834eab83b1df539f271871",
            "336a8f1a243c44158005e498950d0910",
            "b9e6cd3672614d4eaeaf138212d99cae",
            "51dacc2113bb4753b81e33acd5380798",
            "b2db9f0f453e484db5273238d47c4dca",
            "18c02e55f2b44e948037564dc7da5fee",
            "fa99af2d923c43c0b960c3f16a858fea"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtering train and test data by month...\n",
            "Train data for Dec, Jan, Feb: 2859 rows\n",
            "Test data for Jan, Feb: 564 rows\n",
            "\n",
            "Processing train data and collecting tiles...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Tiles:   0%|          | 0/2859 [00:00<?, ?file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2a56b85ab21428c8c3d5c577d114596"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 2859 tiles.\n",
            "\n",
            "Processing test data and collecting tiles...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Tiles:   0%|          | 0/564 [00:00<?, ?file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04ab451cc1a54ca4a7da384bb600d7a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed 564 tiles.\n",
            "\n",
            "Shape of train_processed_df: (2859, 7)\n",
            "Shape of train_tiles_array: (2859, 12, 326, 326)\n",
            "Shape of train_labels: (2859,)\n",
            "\n",
            "Shape of test_processed_df: (564, 4)\n",
            "Shape of test_tiles_array: (564, 12, 326, 326)\n",
            "Shape of test_labels: None\n",
            "\n",
            "Extracting features for train data...\n",
            "Starting feature extraction for 2859 tiles...\n",
            "Computing vegetation and other indices...\n",
            "Calculating band and index statistics...\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#################"
      ],
      "metadata": {
        "trusted": true,
        "id": "1-3ovKj_pW94"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9HDVflj9rIg0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}